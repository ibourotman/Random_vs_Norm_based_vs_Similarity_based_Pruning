{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_9R7-eAPUUy",
        "outputId": "5861b073-2d52-4ac5-c216-64a64be67bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rAF9pAvYgC2",
        "outputId": "d71c2431-d189-4d0d-f111-4b97e94a66e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define VGG-16-BN model"
      ],
      "metadata": {
        "id": "bvA-fbFDPq9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "defaultcfg = [\n",
        "    64,\n",
        "    64,\n",
        "    \"M\",\n",
        "    128,\n",
        "    128,\n",
        "    \"M\",\n",
        "    256,\n",
        "    256,\n",
        "    256,\n",
        "    \"M\",\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    \"M\",\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "]\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, compress_rate=[0.0] * 13, cfg=None, num_classes=10):\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        if cfg is None:\n",
        "            cfg = defaultcfg\n",
        "\n",
        "        self.compress_rate = compress_rate[:]\n",
        "\n",
        "        self.features = self._make_layers(cfg)\n",
        "        last_conv_out_channels = self.features[-3].out_channels\n",
        "        self.classifier = nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\"linear1\", nn.Linear(last_conv_out_channels, cfg[-1])),\n",
        "                    (\"norm1\", nn.BatchNorm1d(cfg[-1])),\n",
        "                    (\"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\"linear2\", nn.Linear(cfg[-1], num_classes)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = nn.Sequential()\n",
        "        in_channels = 3\n",
        "        cnt = 0\n",
        "\n",
        "        for i, x in enumerate(cfg):\n",
        "            if x == \"M\":\n",
        "                layers.add_module(\"pool%d\" % i, nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            else:\n",
        "                x = int(x * (1 - self.compress_rate[cnt]))\n",
        "                cnt += 1\n",
        "                conv2d = nn.Conv2d(in_channels, x, kernel_size=3, padding=1)\n",
        "                layers.add_module(\"conv%d\" % i, conv2d)\n",
        "                layers.add_module(\"norm%d\" % i, nn.BatchNorm2d(x))\n",
        "                layers.add_module(\"relu%d\" % i, nn.ReLU(inplace=True))\n",
        "                in_channels = x\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def vgg_16_bn(compress_rate=[0.0] * 13):\n",
        "    return VGG(compress_rate=compress_rate)"
      ],
      "metadata": {
        "id": "PL0WKYG_PYg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "BsmPZQRgQb2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_cpr(compress_rate):\n",
        "    cprate_str = compress_rate\n",
        "    cprate_str_list = cprate_str.split(\"+\")\n",
        "    pat_cprate = re.compile(r\"\\d+\\.\\d*\")\n",
        "    pat_num = re.compile(r\"\\*\\d+\")\n",
        "    cprate = []\n",
        "    for x in cprate_str_list:\n",
        "        num = 1\n",
        "        find_num = re.findall(pat_num, x)\n",
        "        if find_num:\n",
        "            assert len(find_num) == 1\n",
        "            num = int(find_num[0].replace(\"*\", \"\"))\n",
        "        find_cprate = re.findall(pat_cprate, x)\n",
        "        assert len(find_cprate) == 1\n",
        "        cprate += [float(find_cprate[0])] * num\n",
        "\n",
        "    return cprate"
      ],
      "metadata": {
        "id": "stzE4MnmQE1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import time, datetime\n",
        "import logging\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "\n",
        "\n",
        "'''record configurations'''\n",
        "class record_config():\n",
        "    def __init__(self, args):\n",
        "        now = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "        today = datetime.date.today()\n",
        "\n",
        "        self.args = args\n",
        "        self.job_dir = Path(args.job_dir)\n",
        "\n",
        "        def _make_dir(path):\n",
        "            if not os.path.exists(path):\n",
        "                os.makedirs(path)\n",
        "\n",
        "        _make_dir(self.job_dir)\n",
        "\n",
        "        config_dir = self.job_dir / 'config.txt'\n",
        "        #if not os.path.exists(config_dir):\n",
        "        if args.resume:\n",
        "            with open(config_dir, 'a') as f:\n",
        "                f.write(now + '\\n\\n')\n",
        "                for arg in vars(args):\n",
        "                    f.write('{}: {}\\n'.format(arg, getattr(args, arg)))\n",
        "                f.write('\\n')\n",
        "        else:\n",
        "            with open(config_dir, 'w') as f:\n",
        "                f.write(now + '\\n\\n')\n",
        "                for arg in vars(args):\n",
        "                    f.write('{}: {}\\n'.format(arg, getattr(args, arg)))\n",
        "                f.write('\\n')\n",
        "\n",
        "\n",
        "def get_logger(file_path):\n",
        "\n",
        "    logger = logging.getLogger('gal')\n",
        "    log_format = '%(asctime)s | %(message)s'\n",
        "    formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "    file_handler = logging.FileHandler(file_path)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(stream_handler)\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    return logger\n",
        "\n",
        "#label smooth\n",
        "class CrossEntropyLabelSmooth(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes, epsilon):\n",
        "    super(CrossEntropyLabelSmooth, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.epsilon = epsilon\n",
        "    self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "    log_probs = self.logsoftmax(inputs)\n",
        "    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\n",
        "    targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
        "    loss = (-targets * log_probs).mean(0).sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print(' '.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, save):\n",
        "    if not os.path.exists(save):\n",
        "        os.makedirs(save)\n",
        "    filename = os.path.join(save, 'checkpoint.pth.tar')\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        best_filename = os.path.join(save, 'model_best.pth.tar')\n",
        "        shutil.copyfile(filename, best_filename)\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "\n",
        "def progress_bar(current, total, msg=None):\n",
        "    _, term_width = os.popen('stty size', 'r').read().split()\n",
        "    term_width = int(term_width)\n",
        "\n",
        "    TOTAL_BAR_LENGTH = 65.\n",
        "    last_time = time.time()\n",
        "    begin_time = last_time\n",
        "\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "metadata": {
        "id": "7MuvmxS1Sa-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, train_loader, model, criterion, optimizer, scheduler):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    cur_lr = optimizer.param_groups[0]['lr']\n",
        "    print('learning_rate: ' + str(cur_lr))\n",
        "\n",
        "    num_iter = len(train_loader)\n",
        "    print_freq = num_iter // 10\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n",
        "        n = images.size(0)\n",
        "        losses.update(loss.item(), n)  # accumulated loss\n",
        "        top1.update(prec1.item(), n)\n",
        "        top5.update(prec5.item(), n)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print(\n",
        "                'Epoch[{0}]({1}/{2}): '\n",
        "                'Loss {loss.avg:.4f} '\n",
        "                'Prec@1(1,5) {top1.avg:.2f}, {top5.avg:.2f} '\n",
        "                'Lr {cur_lr:.4f}'.format(\n",
        "                    epoch, i, num_iter, loss=losses,\n",
        "                    top1=top1, top5=top5, cur_lr=cur_lr))\n",
        "    scheduler.step()\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            pred1, pred5 = accuracy(logits, target, topk=(1, 5))\n",
        "            n = images.size(0)\n",
        "            losses.update(loss.item(), n)\n",
        "            top1.update(pred1[0], n)\n",
        "            top5.update(pred5[0], n)\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "                    .format(top1=top1, top5=top5))\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg"
      ],
      "metadata": {
        "id": "QpORMFk9SFsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def load_data(batch_size=128):\n",
        "\n",
        "    # load training data\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.CIFAR10(root=\"./\", train=True, download=True,\n",
        "                                            transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR10(root=\"./\", train=False, download=True, transform=transform_test)\n",
        "    val_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "wOEgSgHfRYBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "epochs = 100\n",
        "lr_warmup_epochs=5\n",
        "lr=0.01\n",
        "momentum=0.9\n",
        "weight_decay=5e-4\n",
        "lr_warmup_decay=0.01"
      ],
      "metadata": {
        "id": "85hVYLlKVtUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune(model, train_loader, val_loader, epochs, criterion):\n",
        "    optimizer = torch.optim.SGD(model.parameters(\n",
        "    ), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    main_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=epochs-lr_warmup_epochs)\n",
        "    warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer, start_factor=lr_warmup_decay, total_iters=lr_warmup_epochs)\n",
        "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "        optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[lr_warmup_epochs])\n",
        "\n",
        "    _, best_top1_acc, _ = validate(val_loader, model, criterion)\n",
        "    best_model_state = copy.deepcopy(model.state_dict())\n",
        "    epoch = 0\n",
        "    while epoch < epochs:\n",
        "        train(epoch, train_loader, model, criterion,\n",
        "              optimizer, scheduler)\n",
        "        _, valid_top1_acc, _ = validate(val_loader, model, criterion)\n",
        "\n",
        "        if valid_top1_acc > best_top1_acc:\n",
        "            best_top1_acc = valid_top1_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "        epoch += 1\n",
        "        print('=>Best accuracy {:.3f}'.format(best_top1_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "tqqne4ZjX5ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 3: Load the pretrained baseline model**"
      ],
      "metadata": {
        "id": "L9DHHosHQ0KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/pvtien96/CORING/releases/download/v0.1.0/vgg_16_bn.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuzsnEEyV7o1",
        "outputId": "15a47ee2-e568-4b4e-ddf7-0af636000c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-07 21:54:21--  https://github.com/pvtien96/CORING/releases/download/v0.1.0/vgg_16_bn.pt\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/572465934/6bb9aca3-1335-40ce-8a25-df08be78e4eb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240607%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240607T215422Z&X-Amz-Expires=300&X-Amz-Signature=8a0967615e46d1faa698e14ad0bc80d8521081574b7bc21b495c2b478933e307&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=572465934&response-content-disposition=attachment%3B%20filename%3Dvgg_16_bn.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-06-07 21:54:22--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/572465934/6bb9aca3-1335-40ce-8a25-df08be78e4eb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240607%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240607T215422Z&X-Amz-Expires=300&X-Amz-Signature=8a0967615e46d1faa698e14ad0bc80d8521081574b7bc21b495c2b478933e307&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=572465934&response-content-disposition=attachment%3B%20filename%3Dvgg_16_bn.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119998475 (114M) [application/octet-stream]\n",
            "Saving to: ‘vgg_16_bn.pt’\n",
            "\n",
            "vgg_16_bn.pt        100%[===================>] 114.44M  44.3MB/s    in 2.6s    \n",
            "\n",
            "2024-06-07 21:54:25 (44.3 MB/s) - ‘vgg_16_bn.pt’ saved [119998475/119998475]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import copy\n",
        "\n",
        "\n",
        " # initialize model\n",
        "model_ori = vgg_16_bn(compress_rate=[0.0]*13).cuda()\n",
        "print(model_ori)\n",
        "\n",
        "# load training data\n",
        "train_loader, val_loader = load_data()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# load the baseline model\n",
        "checkpoint = torch.load(\"./vgg_16_bn.pt\", map_location=torch.device('cuda:0'))\n",
        "model_ori.load_state_dict(checkpoint['state_dict'])\n",
        "\n"
      ],
      "metadata": {
        "id": "R7LQVpYnQzYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bcc3e5-a1c0-4a74-bcbd-ec57eb9a3469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(inplace=True)\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(inplace=True)\n",
            "    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(inplace=True)\n",
            "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(inplace=True)\n",
            "    (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(inplace=True)\n",
            "    (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(inplace=True)\n",
            "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(inplace=True)\n",
            "    (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(inplace=True)\n",
            "    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(inplace=True)\n",
            "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(inplace=True)\n",
            "    (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(inplace=True)\n",
            "    (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29847215.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the baseline model:\")\n",
        "_, accuracy_model_ori, _ = validate(val_loader, model_ori, criterion)\n",
        "print(f\"This model's accuracy is {accuracy_model_ori}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfdZoOCaX68u",
        "outputId": "4690b018-3fb1-4475-d3ff-25cefbea66a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the baseline model:\n",
            " * Acc@1 93.960 Acc@5 99.730\n",
            "This model's accuracy is 93.95999908447266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amiWLjJEYHYO",
        "outputId": "39cb4612-c9a1-43b1-c5f8-431e69239f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->ptflops) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ptflops import get_model_complexity_info\n",
        "with torch.cuda.device(0):\n",
        "  macs, params = get_model_complexity_info(model_ori, (3, 32, 32), as_strings=False, print_per_layer_stat=True, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zta0pSkEX_Ce",
        "outputId": "e580322c-b214-45c1-98dc-50a2a876416e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  14.99 M, 100.000% Params, 314.69 MMac, 99.872% MACs, \n",
            "  (features): Sequential(\n",
            "    14.72 M, 98.207% Params, 314.43 MMac, 99.787% MACs, \n",
            "    (conv0): Conv2d(1.79 k, 0.012% Params, 1.84 MMac, 0.582% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(128, 0.001% Params, 131.07 KMac, 0.042% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(0, 0.000% Params, 65.54 KMac, 0.021% MACs, inplace=True)\n",
            "    (conv1): Conv2d(36.93 k, 0.246% Params, 37.81 MMac, 12.001% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(128, 0.001% Params, 131.07 KMac, 0.042% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 65.54 KMac, 0.021% MACs, inplace=True)\n",
            "    (pool2): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.021% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(73.86 k, 0.493% Params, 18.91 MMac, 6.000% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(256, 0.002% Params, 65.54 KMac, 0.021% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (conv4): Conv2d(147.58 k, 0.984% Params, 37.78 MMac, 11.990% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(256, 0.002% Params, 65.54 KMac, 0.021% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (pool5): MaxPool2d(0, 0.000% Params, 32.77 KMac, 0.010% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(295.17 k, 1.969% Params, 18.89 MMac, 5.995% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(512, 0.003% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (conv7): Conv2d(590.08 k, 3.936% Params, 37.77 MMac, 11.985% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(512, 0.003% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (conv8): Conv2d(590.08 k, 3.936% Params, 37.77 MMac, 11.985% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(512, 0.003% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (pool9): MaxPool2d(0, 0.000% Params, 16.38 KMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(1.18 M, 7.872% Params, 18.88 MMac, 5.993% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(0, 0.000% Params, 8.19 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv11): Conv2d(2.36 M, 15.741% Params, 37.76 MMac, 11.983% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(0, 0.000% Params, 8.19 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv12): Conv2d(2.36 M, 15.741% Params, 37.76 MMac, 11.983% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(0, 0.000% Params, 8.19 KMac, 0.003% MACs, inplace=True)\n",
            "    (pool13): MaxPool2d(0, 0.000% Params, 8.19 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(2.36 M, 15.741% Params, 9.44 MMac, 2.996% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(1.02 k, 0.007% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv15): Conv2d(2.36 M, 15.741% Params, 9.44 MMac, 2.996% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(1.02 k, 0.007% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv16): Conv2d(2.36 M, 15.741% Params, 9.44 MMac, 2.996% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(1.02 k, 0.007% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    268.81 k, 1.793% Params, 269.32 KMac, 0.085% MACs, \n",
            "    (linear1): Linear(262.66 k, 1.752% Params, 262.66 KMac, 0.083% MACs, in_features=512, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(1.02 k, 0.007% Params, 1.02 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)\n",
            "    (linear2): Linear(5.13 k, 0.034% Params, 5.13 KMac, 0.002% MACs, in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of parameter and MACs of this model are {params} and {macs}, respectively.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPGni3I0YfYX",
        "outputId": "a23f4536-1f07-47b9-d7ca-a6778d0ee679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of parameter and MACs of this model are 14991946 and 315096586, respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pU7P10nGFBb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\# ** 3 methods to prune the model**\n",
        "\n",
        "*   Random\n",
        "*   Norm-based\n",
        "*   Distance-based\n",
        "\n"
      ],
      "metadata": {
        "id": "6pFryMvGXX9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compress_rate = [0.25]*13 # prune 25% of all layers\n",
        "model_prune = vgg_16_bn(compress_rate=compress_rate).cuda()\n",
        "print(model_prune)\n"
      ],
      "metadata": {
        "id": "SxxynwaVXbp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb313754-b199-4f88-f7d6-5d0eac135782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(inplace=True)\n",
            "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(inplace=True)\n",
            "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(inplace=True)\n",
            "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(inplace=True)\n",
            "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(inplace=True)\n",
            "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(inplace=True)\n",
            "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(inplace=True)\n",
            "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(inplace=True)\n",
            "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(inplace=True)\n",
            "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(inplace=True)\n",
            "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(inplace=True)\n",
            "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ** Random**\n"
      ],
      "metadata": {
        "id": "igXpB54LvK3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_random(model, model_ori):\n",
        "    oristate_dict = model_ori.state_dict()\n",
        "    state_dict = model.state_dict()\n",
        "    last_select_index = None  # Conv index selected in the previous layer\n",
        "\n",
        "    cnt = 0\n",
        "    for name, module in model.named_modules():\n",
        "        name = name.replace('module.', '')\n",
        "\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            cnt += 1\n",
        "            oriweight = oristate_dict[name + '.weight']\n",
        "            curweight = state_dict[name + '.weight']\n",
        "            orifilter_num = oriweight.size(0)\n",
        "            currentfilter_num = curweight.size(0)\n",
        "            print(f\"Processing layer {cnt}, original layer has {orifilter_num} filters, pruning model has {currentfilter_num} filters\")\n",
        "\n",
        "\n",
        "            if orifilter_num != currentfilter_num:\n",
        "                cov_id = cnt\n",
        "                #************ rank the filter's importance here\n",
        "                rank = np.arange(1, orifilter_num + 1)\n",
        "                np.random.shuffle(rank)\n",
        "                #********************\n",
        "                print(f\"rank {rank}\")\n",
        "                select_index = np.argsort(\n",
        "                    rank)[orifilter_num-currentfilter_num:]  # preserved filter id\n",
        "                select_index.sort()\n",
        "\n",
        "                if last_select_index is not None:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        for index_j, j in enumerate(last_select_index):\n",
        "                            state_dict[name + '.weight'][index_i][index_j] = \\\n",
        "                                oristate_dict[name + '.weight'][i][j]\n",
        "                else:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        state_dict[name + '.weight'][index_i] = \\\n",
        "                            oristate_dict[name + '.weight'][i]\n",
        "\n",
        "                last_select_index = select_index\n",
        "\n",
        "            elif last_select_index is not None:\n",
        "                for i in range(orifilter_num):\n",
        "                    for index_j, j in enumerate(last_select_index):\n",
        "                        state_dict[name + '.weight'][i][index_j] = \\\n",
        "                            oristate_dict[name + '.weight'][i][j]\n",
        "            else:\n",
        "                state_dict[name + '.weight'] = oriweight\n",
        "                last_select_index = None\n",
        "\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "BzcvypUwFx8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_random(model_prune, model_ori)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYPThKmdgioW",
        "outputId": "2618ffa7-7d22-4a12-cc39-43701f36bd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing layer 1, original layer has 64 filters, pruning model has 48 filters\n",
            "rank [59 56 62  8 44 43 27 45 29  1 39 22 16 20 24 10  6 26 36 31 41  3 23 37\n",
            " 55 57  4  9 58 21 13 49 40  2 42 14 46 28  7 30 60 52 50 18 61  5 48 53\n",
            " 15 12 64 11 32 38 35 51 33 63 25 54 47 17 34 19]\n",
            "Processing layer 2, original layer has 64 filters, pruning model has 48 filters\n",
            "rank [41 13 17 23 20 21 44 49 35 63 56 61 10 47 12 53  8 42 31 32 48 39 58 29\n",
            " 27 62 37 52 22 33 16 55 43 50  1 11 64 40 30 34 38  2 51  9 59  5 15 25\n",
            " 60 26 36 14 19 24  7 18 57 28  4 45 54  3  6 46]\n",
            "Processing layer 3, original layer has 128 filters, pruning model has 96 filters\n",
            "rank [ 88  41 107  37  67  95  76  32   2  26  89 124 105  18  59  25 118  16\n",
            "  23  14  15  83 104  82  96  13 109  80 123 119  63  43  34  92 127  84\n",
            "  62  17 102  53  55  61 113  29  44  94  78 101 116  19  77  98  99 121\n",
            "  49   9  28  33  81  24  70  86  42  75  11   5  69  90 125  31 117   6\n",
            "  40  35  60  72  66  20  85  57  74  36  79  21   4 106 115  71  97  38\n",
            "  30 110 100  22 112  52  48 122  10  93  12  47  65  27 111   1  58  46\n",
            "  39  87  73  91   8 120  51 126 108 103  54  68  56 114 128  64  45   3\n",
            "  50   7]\n",
            "Processing layer 4, original layer has 128 filters, pruning model has 96 filters\n",
            "rank [ 67 110  45  97  88  99  34  66  73  75  25 108  84 120  17 112  61 107\n",
            " 116  69  83  33  13  27  79  85  22  44  15  62  11  38  21  31  63  56\n",
            "  81 102   4  24  71   1 126 101 123 117  12  77  76 106  82   6  51  93\n",
            "  91  10 115  35  72 121  92 125  30  37  36  43   2  98  74  80   5  20\n",
            "  68  29  90  49  87  40  89 119  58  55 124  46 122  19  41 127   7  42\n",
            "  14  96 103  26  52  47  94  78 105  16 104  57  95 114 111 113 128   8\n",
            "  65 100  18  59  60  64  54  28  32  50  48  39   3  70  53 109  86 118\n",
            "   9  23]\n",
            "Processing layer 5, original layer has 256 filters, pruning model has 192 filters\n",
            "rank [118  14 187 144 242  17 221 142 239 155   2 243 107   7 198 241 249 215\n",
            "  75 110  96 101  35 100 256 174 236 186  43  56 105   6  28   1  76 108\n",
            " 116 125 140  55  70 145 119 225 143  40  50  12 182  16 134 162  97 190\n",
            "  61 246 227 123 126 132 153 139 128 247  36 133  32 167 235 117 163  34\n",
            " 255  62 233 166 103  59 183  80  46 222 196 219   4 206 209  48  79 151\n",
            "  84 203 216  11  25  88  77 244  91 124  53  15 184 238 213 205 234 201\n",
            " 161 211 223 177 253 232 137  63 195 104  51 214 178 185 251 180 197  73\n",
            " 250 226 207 129  99  72 113 208 240 154 173  31 112 224  41  95 121 164\n",
            "   9  66 131  86  68  21 114  13   8  52  44 179 122  24 175 237 111 202\n",
            "  65  64  37 210  54  27 127 245  30 189 192  20 231  83 141  69 248   3\n",
            "  47  90  33 152 147 218 130 150 204  45 109 156 176 191  26 254 172  82\n",
            " 165  78  19  71 149 194 229  81  93  94 120 106 158  23  74 115 168  58\n",
            " 228  22 220  60  57  10  39 146 212 217  85 157 230 138 160 148  89 171\n",
            " 200  18 135  87  49 193 169 252 159 199 170  38   5 181  42  98 188  92\n",
            " 136  67 102  29]\n",
            "Processing layer 6, original layer has 256 filters, pruning model has 192 filters\n",
            "rank [160 254 155 243  62 125  81 124 234  13  60   8 252 105   9  64  32  25\n",
            "  47  76 164  29 120 154  74 253  59  58  35 127 224 182 171  99 147 178\n",
            " 217 144 111 207 115  23 221 211 225   5 208  82 156 198  27 237 185 200\n",
            " 141 126  28  31 232 240 117 215 136  45 153  11 159 114 245  51  71  44\n",
            " 183  67  85 138 175 122  49 149 255 206 220 201 214 249 209 196 239 247\n",
            "  34  39  54 118  18  53 233  15 169 103 139 204  16 191 251  30 181   3\n",
            "  92 194  79 152 189 218 143 256 180  46 199 150 162 132 130 188  55 140\n",
            "  33 186 104 222   7  19  80 190 230  75  48 202 195 227 134  12 205 228\n",
            " 248  98 167 184  87  61 151  37  43  70  86  22 148 166 192   1 161 210\n",
            "  26 145 170  50  84 216  36 241 168  72 116 133  56 212 223 235  69  68\n",
            " 101 187  97  52 250  17 193 142 231  78 246  93 135  95  40 203  73 226\n",
            " 119 176  89 131  83 121  65  90  24 173 106 244   6 128 236   2 172 242\n",
            "   4 102 110 109 100  42 213 129 146 123 107 197 229  96 179 177 137 238\n",
            "  66 112  88  38 108  21  94 165  20 157  10 219  57 158  14 113  77  91\n",
            "  41 174  63 163]\n",
            "Processing layer 7, original layer has 256 filters, pruning model has 192 filters\n",
            "rank [218  91 185 247 166 242 121 212  46  97  68 232  77  29  65  74 103 222\n",
            "  78 117 183  98 198 102  73 182 158 108 220  86   9 101 195  72  24 110\n",
            " 156  96 151 118 114  82 197  50 202 250 179 196  28 231 237 255 224 211\n",
            " 180  79  11 126  30 162 137 148  99  58 200 174  25  19 116 123 165  87\n",
            " 204  60 124  14 106 181 122 171 169   8  67 111  42   6 133  75  83 193\n",
            " 241  61 207  49 157 248  34 159 154 254  80  52 163  31  45 115  18 104\n",
            "  53  69 253 175 135 188  90 217   5 184 128 228 251 186 100 213  27 219\n",
            " 149  55 203  54 210 240 120  94 209  12 109 147 168 172 216  66 233 173\n",
            " 141  57 201  85  39  10 194  41   4  43  44 113 119 177  64 199 178  84\n",
            " 127 160  95 190 239 187  51  93  22 234 225  70 176 167 107 129 227  36\n",
            " 245  59  56 189 150 142  71 226  23 105 206 143  35  13 229  21 152 131\n",
            "  33  76 146  88  62  17  20  15 244 170  63 256 139   3 205  47 252 145\n",
            " 144 236 223 238 132 246  48 249 112 125 153  89 235 243 230 191 140 155\n",
            "  81   2   1 161 130 134  92 214 164 215  26 192  37  40 221 208  32 138\n",
            "   7  38 136  16]\n",
            "Processing layer 8, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [212 327 223 408  84 146 473 104 168 415 153 302  64 164  79 376 224  57\n",
            "  34 432 429 140 303 301 250 160 122 328 428  56 314 264 471 492 353 197\n",
            " 220 117 416 267 182 142 500  90 378 126 510 235 442  98 448 324 354 135\n",
            "  81 403 501  16 504  65 102 394 229 233 127 508 130  46 187 417 426 150\n",
            " 436 418 344 204 134 232 258  35  54  38 424 243 219 506 395 342 190 300\n",
            " 493 388  29 285 276 133 463 278 380 114 462 440 131 144 454 107 322  91\n",
            "  74 370  10 470  80 315 234 356 456 244 139 147 308 465 309 366 419 156\n",
            " 294 396   6 373 427 207 271 136 509  18 348 412  44 316 334 405 111 263\n",
            " 198  92 382 486 385 129 287 209 455 286 228  32 397 460  12 143 349 202\n",
            " 259 137 238 172 435 481 123 260  36  66 407 221 360 421  87 505 100 364\n",
            " 236 469   1 145  53 237 363 377 118 338  22 109 375 247 485 431 218 337\n",
            "  59 355 497  82 443  48 333 489 216  41 350 346 331   8 119 284 298 230\n",
            " 268 196  78 445 188 275 359 369 313 290 217 511 402 256 499 387 461 444\n",
            "  33 437 210 390 283 434 502 326  17 345 483 474 148  45 174 401 254 311\n",
            " 341 103 189 410 368 231 458 449 124   9 491  86  58 347 404 450  40 273\n",
            " 389  89 494 365  47 398 165  28 288  23 125 167 319  96 108  19 438 110\n",
            " 257 253  70   5  39  62 120 383  83 478 161 206 282 166  31 227 173 357\n",
            " 248  60  30 195  95 213 178 295 199 361 177 149 176 266 159  85 116 214\n",
            "  27  55 379 171 496 279 411 381 245 296 115 318 251 451 310  93  61 446\n",
            " 194   7 367  37 453  52 280 336 479  11 170 179 242 215 270   3 464 281\n",
            " 181 480 222 184  43   2 141 252 169 249 325  69 163 191 413 317 193 351\n",
            " 132 183 420   4 475 400 482 113 304 293 472 340 414 261 241 332 205  73\n",
            " 457 507  71  63 503 321 297 512 399 175 246  25 208 335 441 490 430 406\n",
            " 272 138 226 423 225 392  21 152 155 240  97 452 320 447 158 255 467 391\n",
            " 200 269 265  99 277  24  94 409 372 112 439 484 262 386 201 371  26  49\n",
            " 289 323 459  51  15 312 121 211 374  13 466 291 192 343 157 498 362  77\n",
            " 495 477 154 306 488 128 185  75  20 476 162  72 186 339 105 203  42  50\n",
            " 239 393 292 151 101  14 330 468 329 358 307 487  88 425  67 299  76 433\n",
            " 106 274 422 180 384  68 305 352]\n",
            "Processing layer 9, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [376 458 268  46 282 430 160  25 271 362 265  91 213  31 511 463  49 482\n",
            " 410  12 261  61 478   1 102 155 120 208 152 236 203 134 262  19  50  54\n",
            "  33 186  41  22 306 178 433 417 200 122 499 464 476 396 391 202 174 222\n",
            " 284  18 353 248 138 176  80 130 169  23 266 343 116  81 378 225 283 354\n",
            " 377 244  94 128 342 161 207 197  82 443 273 455 193 383 177 452 497 325\n",
            " 215 110 162 403 349 421 322 450 219 239 305 242 361  40 136 211 489 191\n",
            " 453 451 299 505  47 407 188 498 294 405 481 418 394 360  10 484 363 190\n",
            "  35 156 280 210 446 258 401 151 170 427 183 495 428 275 486 153 251  38\n",
            " 415 370 460 274  26 364  89 373 512 422   4 462 246 402 263 255 356 472\n",
            " 508 425  99 426 293 139 298  92 310 311 119 220 329  48 230 487 117 289\n",
            " 232 475 399 494 317 384 504  84  53  87 442 315 101 133 292 406 175 241\n",
            " 146 507 393 243  79 267 333  93 124 404 209 439 509 456 398  37  71 395\n",
            " 233 192 111 423  36  69 400 387 144  34 331 385 444 390 112 414 502 367\n",
            " 319 195 140 227 123 276 157 485  58 420 228 108 358 313  16 167  83  39\n",
            " 296 214 328 346 277 173 379 380 351  45  68 125 288 309 382 483 285 429\n",
            " 168 187 372 501 467 492 339 257 121 465 327 473  85 180 436 212  60 237\n",
            " 413 272 348  32 355  21 344 127 386 194  55  56 477  75 252 259 182 126\n",
            " 147 318 438 221 270   5 132 199 469 506 179  24 204 459 316 337 334 131\n",
            " 141 332 159  73 457 226 461 172  44 468 470 290 297 295 448  66  17  97\n",
            "   9 480 419 254 264 105 247 103 496 479 445  76 411 466 216 245 109 308\n",
            " 150  20 135 366 345  59  29 389 250 449 371 326 234 359  65 229 424   6\n",
            " 107 104  88  96 338 434 381 490 238 115 352 154  86 488  62 100  52 335\n",
            "  64 281 408  67 392 148 113 388 164 287 223  42 369 278 184 323 158  95\n",
            " 320 231 397 206 145 189 440   3  13 163 300 431 260 307 412   8  78 503\n",
            " 114  14 301 474 500 304 218  11 350 118 240 374 330 143 347  51 201 365\n",
            " 279 314 302 375 224 196  63  70 171 256 149 198 437 249 142 235  77  30\n",
            "  74 129  27 181 205  72 510 137 166   2 441  28 185 357 340 269 471  90\n",
            " 165  43 493 491 286 253 454 341  15 416 435 303   7 106 291 321 217 447\n",
            " 368  98 312 432 409  57 336 324]\n",
            "Processing layer 10, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [441 157 105 244 477 318 354 400 209 152 268 320 114 226 328 512 312 379\n",
            " 283 375 411 391 224 419 175 298 342  33 227 165  17 497 334 125 363 365\n",
            " 395 138  40  54 237  41  61 119  18 323 100 472 273  75 469 231 194 483\n",
            " 257 330 476 174  20 359 169 337  46 110 421 242 351 352  77 232  38 197\n",
            " 357 132 282 122 263 345  98 426 444  76 420 241 135 377  64  49 259 509\n",
            " 463 186 478 475 435 113  26 269 220 392 341 182 161 145 279 505 108 429\n",
            "  47 506 492 331  60 136  19 491  55  67 402  48 464 277 288 211 262 167\n",
            " 184  29 495 170 332 252 247 253 348  22 218  44  81 493 196 434 372 124\n",
            " 442 198 296 485 457 302 313 340 381 203 369  43 245 200 450 216 128 109\n",
            " 479 418 192 407 126 321 378  65 393  12 131 308 453 438 389 500  73 425\n",
            " 111 164 214  57 386   3 433 292 349 306 317 271 449  59  11 129 267 176\n",
            "  53 346  39 356 446 329 295 204  30 235 188 290 260 159 127 314 388 183\n",
            " 221  24 504 366 384 301 210 264 180 427 307 322  88 387   1 362  45 230\n",
            " 303 172 338 399  10  93 250 228 316  94 486 361 428 281 285 371   9 311\n",
            " 423 240  80 141 168 454  99 103  90 217 304 150  36 207 120 490  62 208\n",
            "  42  97 461 460 187 149 489 458 415 101 146 171 201 467 443 142 408 294\n",
            " 319 355 358  70  34 158 385 430 185   5 223 396 123 382 414 422 137 195\n",
            " 510  31 280 274 508 248 219 199 499 347 431 374 222 488 353 459 190 151\n",
            " 258  14 229 360 215 275 350 153   6  27 144  58 390 405 179  82 134 286\n",
            " 502 397 470 481  25 243  71 465  68  74 309 324 315 333 466 193 455 413\n",
            " 173  83 251 436 233 370 344 336 284 246  63 445 116  95 254  87 160 452\n",
            "  79  84 471 236 293 265 417 501 451 503 287 121 437 339 511 115 327 256\n",
            " 139 234 403 376 178  16 480 507 410 482 487 383 261 439 117 305 326 133\n",
            " 249   8 130 462 394 266  21 156 447 238  69 406 367 368 297 104 424 299\n",
            "  37  35 107 310 473 468 498 155 456  50 278   7 181 448  32  13 212 300\n",
            " 163 401  78 412 276 496  72 140 162  89 225 205 494 270 335 440  51  28\n",
            "  85 474 325 166 147 380  52 177 102 202 106 398   2  66   4 404 291 206\n",
            "  23  91  15  56 148 373 118 272 255 364  96  92 416 239 189 343 484 432\n",
            " 112  86 213 191 409 143 289 154]\n",
            "Processing layer 11, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [155 232  30 120 405  46 349 225 389 151 182 295 369  83  89 325  93 468\n",
            " 334 321 421 356  87 237 499  68 311 472 301 359 402 230 490  45 376 184\n",
            "  90 243 414 234  35 502 201 285 415 133 509 222 293  29 204 458 108 394\n",
            "  86 351 448 442 363 429 110 104 450 320  66 216 336 374 129 141  32 408\n",
            " 127 191 419 248 501 101 380 447 403 205 460   3 412 508 209 291 139 298\n",
            " 467 212 323  51 456 299 124 288  23 337  56 495 112 278 342 346 373 481\n",
            " 289 418  88 109 156  40 469 130 122 255 263 238 483 452  97  10  98 121\n",
            " 197 221 202 159 309 287 353 250 417 163  11 172 290 281 161 107 168  26\n",
            "  82 362  99 226 498 147 312 177 332 211 464 153 273 510 131 227 174 275\n",
            " 357 269 445 305 233 111  48 425  71 116 316  61 401 393 231  63 114  54\n",
            " 367   6 208 303 482 358  19  70 188 140 164 361  84 178 424 459  39 505\n",
            " 300 476 169 264  31 143 348 463 497 340 235 302  80 504 479 268  59 253\n",
            " 333 193 491 444  17  21 437 259 449 196 378  55 167 354 160 322  38 200\n",
            " 355 284 274 431 379 246 241 406 106 251  42 475 382 203 331 236  20 158\n",
            " 189 157  57 347 294 432 166 210 194 372 150 265  50 430 214 277  60 256\n",
            " 344 102 258 438 500 441 148  14  75 138 296 485 488 326 318 400  25 292\n",
            " 335 306  72 365 257 470 195 154 190 224 304  43 144 282 228 427 242 487\n",
            "  76 249  79 175 215 245 388 185 280 453 503 105 113 423 338 176 315 440\n",
            " 308 477  34 162 339  13 297 266  94  64 126 324 471 173 180 480 179  18\n",
            " 283 473   4 247   2 387 404 507 426 461 145 407 313 146 328 360 416   7\n",
            " 136 219 117 262 352 181 183 240 229  41 368  95 119  65   8  44  96 435\n",
            " 399 451  37  15  74 307 466  77 115  12 239 165 413 474 455 443 341 364\n",
            " 396  53  78  49 199 494 409 433 478 213 486   9 206 137   1 377 489 446\n",
            " 252 267 123  16  22 142 512  85 152 343 244 370  36  73 383 317 310 103\n",
            " 496  47 484 392  27  91 422 125 345 276 327 279 314 493  28 198 397 411\n",
            " 391 506 261 384 254 457 371 135 454 223 128 171 272 271 386 220 395 410\n",
            " 134  33 207 192 428 420 270 170 462 398 186 330  58 465 100 286 390 436\n",
            " 149 381 375  69   5 385  24 118  62  92 350 260 217 434 511 132  52  81\n",
            " 366 492 319 439 187  67 329 218]\n",
            "Processing layer 12, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [277  91 393 122 256 319  65 109 161 486 320  28 144   4 222 511 300 188\n",
            "  82 205  87 383 218  57 445  66 206 286  55 456 185  67 295 377 196 128\n",
            " 334  19 149 384  59 203 193  21 192 428 235 449 155 417 457 440 112 250\n",
            " 322 298  94 374 129 485 266 479 156 451  35 150 172 387  84  34  41  73\n",
            " 118  25  10  52 493 411 132 470 330 363 207 293 169 141 365 416 239 331\n",
            " 159 153 131 219 178 496 439 419 187 375 316 263   6 135 294 194 120 310\n",
            " 261 348   7 102 154 431 492 186 210 335 301 495 466   5  45 133 162 438\n",
            " 347 248 503 175  62 209 116 234 111 184 323 243 399 232 405 297 396 499\n",
            " 303  61 200 157 420 444 345   8 228 472 179 314  92 434  46 139 267 361\n",
            " 338 450 491 350 104  33 106 247 454 158 262 202 436 501 212 260 502 339\n",
            " 448  63  89  49  30 464 458 312  20 340 341 151 105 378 108 225 477 215\n",
            " 229 198 103  79 113 343 125 500 217 482   9 147 332  40  76 165 183  54\n",
            " 471  48 136 115  43 173 409 433 357 152 414 246 509 281  60 253  50 382\n",
            "  77  75 254 265 167   2  38 497 160 487 121 101 124  44 478  32 412  27\n",
            " 283 508  88 140 255 308  53 459 376 442 505 268 389 292 231  13 288   3\n",
            "  26  70 406 369  86 336 244 117 313 494 337 398 221 407 280 168 504 362\n",
            " 214 195 233 204 473 164 481 171 181 238 401 435 354  36 220 271 208 366\n",
            "  17 126 423 452 326 223  83 394 276 328 272 353 359 311 315 356 148 274\n",
            "  90 176 413 236 119 510 367 386 432  42 460 107 380  37 191  15  23 462\n",
            "  96 199 138 352   1 421 404 333 321 469 278 240 197 426 468 465  69 190\n",
            " 453 273 282 358 461 327 275 372 285 304 424 269 245 408 474 142 257 324\n",
            " 264  80  71 344 447 397 309  81 364 114 484 182 351 355  31 437  93  12\n",
            " 346 290 379 305 483 177 422  74 296 134  16 307 213 230 299 110  14 237\n",
            "  56 418  98 137 480 349  64 443  72 370 180 455 429 381 145 371 441 241\n",
            " 463 127  22  99 174 252 251 490 410 403  39 368 259  68 227 427 284  47\n",
            " 415 488 476 446 430 270 289 475 329 249 242 325  29  95 258 425  51 360\n",
            " 391 512 400 291 123  11 163 402  18 390 467 317  97 170 143 498 100  78\n",
            "  24 130 392 211 302 388 216 146 385 373 342  85 507 395  58 489 287 306\n",
            " 166 224 201 226 279 318 506 189]\n",
            "Processing layer 13, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [ 87 271 136 209 411 139 452 245  59 391  13 303 149 463  40 502 457 326\n",
            " 357 380 112 446 400 218 466 106 496  48  52 412 481 482 220 309  86 205\n",
            " 128 290 421 207 324 296 366 419 239 483 227 254 378 151 360 147 224  53\n",
            "   8 117 441 382 313  56 145 211  14 398 478 293 342 332 422 491 343 315\n",
            " 222 321 404 499 181  39  61 101  45 113 449 143 459 356  57   4 465  78\n",
            " 268 208 348 221  23 127 361 428 140 148 100  83 287 280 276 126  70 418\n",
            " 161 104 248 453 194 273 307 206 242 170 394 114  92 162 305 469 431  22\n",
            " 347 506 212 306 223 216 176 410 408 409 443 129  42  55 327 487 219  97\n",
            " 480  88  34 241 278 337 199 318 232 375 142 341 447 472 302 177  16  18\n",
            " 171 358 426   7 335 420 191 285 445  10 154  63 198 174 292  66 138 325\n",
            "  50 109  38 424 275 444 427 116 167 405 501 423 504 448 471 490 462   9\n",
            " 193 372  27 155 178 121 132   6 395 329 374 369 186 475 153 263 297   3\n",
            " 503 331 282 414  20 108  84 413 505 336 333 474 172 163  62 437 359 289\n",
            " 484 362  95 432  44 253 385 371 150 440 120 184 383 346 257  24 182 363\n",
            "  36 416 164 251 458 160 328 476 468 281 226 387  90 364  67 384 270 210\n",
            " 274  89 300 125 345 204 308  65 229 215 118 236 403 183 340 166  33  19\n",
            " 486 195 301 370 401 435 131 494 330 152  21 107 196  75 197 179  29 169\n",
            "   5 266 497   1 461 390 479 425 119 188 402 294 455 133 436 493 510 137\n",
            " 316  76 105  54 260 265 334  60  46 322 320 355  68 508 451 272 201 399\n",
            "  96 192 368 365 376 187  71 485 247 190 344 489  43 269  91 233 225 286\n",
            " 159 284 111 473 442 259 267 311 351 180 250  74 217  98 495 388 157 252\n",
            " 295  99  35 439 214 407  94 264 134 230 353  25  73  93 249 144 256 389\n",
            " 283 299 464  32 417 122 433 377 173 135 354  58 396 338 350 310  64 165\n",
            " 200 262  47 238 123 492 429 235  81 512   2 213 103  85 130 246 168 298\n",
            " 141 352 158 312 381 507  26 323  30 288 367 255 406  72 240 456 189 115\n",
            " 450 202 454 244  79  80 237 279 397 319 500  37  12 392 231 339  51 102\n",
            " 430 234 291 110  41 243  77  31 314 379 415 488 261  17 349 228 470 386\n",
            "  15 511 373 156 460 124 203  49  11 393 477 175 277  28 146 438 434 185\n",
            " 498 258 467 317 509 304  69  82]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(model_prune, train_loader, val_loader, epochs=1, criterion=criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr2GM056jNaX",
        "outputId": "a28c20d2-bcd1-44c5-c9e7-aa919fbdd35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 10.000 Acc@5 50.000\n",
            "learning_rate: 0.0001\n",
            "Epoch[0](0/391): Loss 2.4927 Prec@1(1,5) 8.59, 46.09 Lr 0.0001\n",
            "Epoch[0](39/391): Loss 2.1735 Prec@1(1,5) 27.79, 64.36 Lr 0.0001\n",
            "Epoch[0](78/391): Loss 1.8697 Prec@1(1,5) 44.94, 77.78 Lr 0.0001\n",
            "Epoch[0](117/391): Loss 1.6595 Prec@1(1,5) 53.45, 83.73 Lr 0.0001\n",
            "Epoch[0](156/391): Loss 1.5107 Prec@1(1,5) 58.09, 86.97 Lr 0.0001\n",
            "Epoch[0](195/391): Loss 1.3916 Prec@1(1,5) 61.68, 89.06 Lr 0.0001\n",
            "Epoch[0](234/391): Loss 1.2990 Prec@1(1,5) 64.31, 90.55 Lr 0.0001\n",
            "Epoch[0](273/391): Loss 1.2259 Prec@1(1,5) 66.35, 91.60 Lr 0.0001\n",
            "Epoch[0](312/391): Loss 1.1641 Prec@1(1,5) 67.92, 92.44 Lr 0.0001\n",
            "Epoch[0](351/391): Loss 1.1081 Prec@1(1,5) 69.45, 93.12 Lr 0.0001\n",
            "Epoch[0](390/391): Loss 1.0614 Prec@1(1,5) 70.69, 93.69 Lr 0.0001\n",
            " * Acc@1 80.220 Acc@5 98.540\n",
            "=>Best accuracy 80.220\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu4): ReLU(inplace=True)\n",
              "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu6): ReLU(inplace=True)\n",
              "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu7): ReLU(inplace=True)\n",
              "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu8): ReLU(inplace=True)\n",
              "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu10): ReLU(inplace=True)\n",
              "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu11): ReLU(inplace=True)\n",
              "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu12): ReLU(inplace=True)\n",
              "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu14): ReLU(inplace=True)\n",
              "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu15): ReLU(inplace=True)\n",
              "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu16): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
              "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.device(0):\n",
        "  macs_prune, params_prune = get_model_complexity_info(model_prune, (3, 32, 32), as_strings=False, print_per_layer_stat=True, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XlC_VuUr-CL",
        "outputId": "7fd3d77c-3199-4b64-ba52-2d77712add60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  8.49 M, 100.000% Params, 177.63 MMac, 99.831% MACs, \n",
            "  (features): Sequential(\n",
            "    8.28 M, 97.605% Params, 177.43 MMac, 99.716% MACs, \n",
            "    (conv0): Conv2d(1.34 k, 0.016% Params, 1.38 MMac, 0.773% MACs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(96, 0.001% Params, 98.3 KMac, 0.055% MACs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(0, 0.000% Params, 49.15 KMac, 0.028% MACs, inplace=True)\n",
            "    (conv1): Conv2d(20.78 k, 0.245% Params, 21.28 MMac, 11.961% MACs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(96, 0.001% Params, 98.3 KMac, 0.055% MACs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 49.15 KMac, 0.028% MACs, inplace=True)\n",
            "    (pool2): MaxPool2d(0, 0.000% Params, 49.15 KMac, 0.028% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(41.57 k, 0.490% Params, 10.64 MMac, 5.981% MACs, 48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(192, 0.002% Params, 49.15 KMac, 0.028% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(0, 0.000% Params, 24.58 KMac, 0.014% MACs, inplace=True)\n",
            "    (conv4): Conv2d(83.04 k, 0.978% Params, 21.26 MMac, 11.947% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(192, 0.002% Params, 49.15 KMac, 0.028% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(0, 0.000% Params, 24.58 KMac, 0.014% MACs, inplace=True)\n",
            "    (pool5): MaxPool2d(0, 0.000% Params, 24.58 KMac, 0.014% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(166.08 k, 1.957% Params, 10.63 MMac, 5.974% MACs, 96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(384, 0.005% Params, 24.58 KMac, 0.014% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(0, 0.000% Params, 12.29 KMac, 0.007% MACs, inplace=True)\n",
            "    (conv7): Conv2d(331.97 k, 3.911% Params, 21.25 MMac, 11.941% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(384, 0.005% Params, 24.58 KMac, 0.014% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(0, 0.000% Params, 12.29 KMac, 0.007% MACs, inplace=True)\n",
            "    (conv8): Conv2d(331.97 k, 3.911% Params, 21.25 MMac, 11.941% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(384, 0.005% Params, 24.58 KMac, 0.014% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(0, 0.000% Params, 12.29 KMac, 0.007% MACs, inplace=True)\n",
            "    (pool9): MaxPool2d(0, 0.000% Params, 12.29 KMac, 0.007% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(663.94 k, 7.822% Params, 10.62 MMac, 5.970% MACs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(768, 0.009% Params, 12.29 KMac, 0.007% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(0, 0.000% Params, 6.14 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv11): Conv2d(1.33 M, 15.640% Params, 21.24 MMac, 11.937% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(768, 0.009% Params, 12.29 KMac, 0.007% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(0, 0.000% Params, 6.14 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv12): Conv2d(1.33 M, 15.640% Params, 21.24 MMac, 11.937% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(768, 0.009% Params, 12.29 KMac, 0.007% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(0, 0.000% Params, 6.14 KMac, 0.003% MACs, inplace=True)\n",
            "    (pool13): MaxPool2d(0, 0.000% Params, 6.14 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(1.33 M, 15.640% Params, 5.31 MMac, 2.984% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(768, 0.009% Params, 3.07 KMac, 0.002% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(0, 0.000% Params, 1.54 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv15): Conv2d(1.33 M, 15.640% Params, 5.31 MMac, 2.984% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(768, 0.009% Params, 3.07 KMac, 0.002% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(0, 0.000% Params, 1.54 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv16): Conv2d(1.33 M, 15.640% Params, 5.31 MMac, 2.984% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(768, 0.009% Params, 3.07 KMac, 0.002% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(0, 0.000% Params, 1.54 KMac, 0.001% MACs, inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    203.27 k, 2.395% Params, 203.79 KMac, 0.115% MACs, \n",
            "    (linear1): Linear(197.12 k, 2.322% Params, 197.12 KMac, 0.111% MACs, in_features=384, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(1.02 k, 0.012% Params, 1.02 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)\n",
            "    (linear2): Linear(5.13 k, 0.060% Params, 5.13 KMac, 0.003% MACs, in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ** Norm**"
      ],
      "metadata": {
        "id": "cDJ03cH7vShO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_norm(model, model_ori):\n",
        "    oristate_dict = model_ori.state_dict()\n",
        "    state_dict = model.state_dict()\n",
        "    last_select_index = None  # Conv index selected in the previous layer\n",
        "\n",
        "    cnt = 0\n",
        "    for name, module in model.named_modules():\n",
        "        name = name.replace('module.', '')\n",
        "\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            cnt += 1\n",
        "            oriweight = oristate_dict[name + '.weight']\n",
        "            curweight = state_dict[name + '.weight']\n",
        "            orifilter_num = oriweight.size(0)\n",
        "            currentfilter_num = curweight.size(0)\n",
        "            print(f\"Processing layer {cnt}, original layer has {orifilter_num} filters, pruning model has {currentfilter_num} filters\")\n",
        "\n",
        "\n",
        "            if orifilter_num != currentfilter_num:\n",
        "                cov_id = cnt\n",
        "                #************ rank the filter's importance here\n",
        "                print(oristate_dict[name + '.weight'].shape)\n",
        "                weight = oristate_dict[name + '.weight'].data\n",
        "                weight = weight.reshape(weight.size(0), weight.size(1)*weight.size(2)*weight.size(3))\n",
        "                norms = torch.norm(weight, dim=1)  # Compute norm along dimensions 1, 2, and 3\n",
        "                print(norms)\n",
        "\n",
        "                # Now, let's rank them based on the norms.\n",
        "                # We'll get the indices that would sort the norms in descending order.\n",
        "                sorted_indices = torch.argsort(norms, descending=True)\n",
        "\n",
        "                # Print the ranks and corresponding norms\n",
        "                for rank, index in enumerate(sorted_indices):\n",
        "                    norm_value = norms[index]\n",
        "                    # print(f\"Rank {rank + 1}: Norm = {norm_value.item()}\")\n",
        "\n",
        "                # If you also want the indices of filters in descending order of their norms\n",
        "                # print(\"Indices of filters in descending order of their norms:\")\n",
        "                # print(sorted_indices)\n",
        "                rank = sorted_indices.cpu().numpy()\n",
        "                #********************\n",
        "                print(f\"rank {rank}\")\n",
        "                select_index = np.argsort(\n",
        "                    rank)[orifilter_num-currentfilter_num:]  # preserved filter id\n",
        "                select_index.sort()\n",
        "\n",
        "                if last_select_index is not None:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        for index_j, j in enumerate(last_select_index):\n",
        "                            state_dict[name + '.weight'][index_i][index_j] = \\\n",
        "                                oristate_dict[name + '.weight'][i][j]\n",
        "                else:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        state_dict[name + '.weight'][index_i] = \\\n",
        "                            oristate_dict[name + '.weight'][i]\n",
        "\n",
        "                last_select_index = select_index\n",
        "\n",
        "            elif last_select_index is not None:\n",
        "                for i in range(orifilter_num):\n",
        "                    for index_j, j in enumerate(last_select_index):\n",
        "                        state_dict[name + '.weight'][i][index_j] = \\\n",
        "                            oristate_dict[name + '.weight'][i][j]\n",
        "            else:\n",
        "                state_dict[name + '.weight'] = oriweight\n",
        "                last_select_index = None\n",
        "\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "4jNru1MUkyzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_norm(model_prune, model_ori)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh5Ed1yTlHOw",
        "outputId": "715ec82d-9e0c-4797-bd31-471dd5367eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing layer 1, original layer has 64 filters, pruning model has 48 filters\n",
            "torch.Size([64, 3, 3, 3])\n",
            "tensor([1.5891e+00, 8.8759e-01, 5.5363e-01, 3.3376e-01, 2.2699e-01, 7.8806e-01,\n",
            "        3.8824e-01, 3.9319e-01, 3.0613e-02, 6.7065e-01, 3.2999e-01, 1.5340e+00,\n",
            "        4.5140e-02, 5.1688e-04, 1.4907e-01, 2.3611e-01, 1.9442e-01, 1.1847e+00,\n",
            "        2.6520e-01, 1.0161e-01, 2.3032e-01, 6.9368e-01, 1.2299e-01, 1.0715e+00,\n",
            "        5.8923e-01, 5.8216e-01, 2.8645e-01, 3.3000e-01, 1.6501e+00, 1.1890e+00,\n",
            "        9.7305e-01, 5.8581e-01, 1.9635e+00, 7.2750e-04, 1.9415e-01, 3.9707e-04,\n",
            "        6.7574e-01, 1.8930e-03, 7.1398e-01, 1.0104e+00, 1.0242e+00, 6.8070e-01,\n",
            "        9.9616e-01, 5.0565e-01, 4.1435e-01, 3.0730e-01, 3.4557e-01, 1.3315e+00,\n",
            "        8.0923e-04, 1.0445e+00, 6.0952e-04, 9.9565e-02, 7.0488e-01, 7.0627e-04,\n",
            "        1.7248e+00, 3.9991e-01, 2.0378e-01, 1.7900e+00, 7.4258e-01, 5.9122e-01,\n",
            "        1.4910e-01, 5.9228e-01, 1.1001e-01, 4.1941e-01], device='cuda:0')\n",
            "rank [32 57 54 28  0 11 47 29 17 23 49 40 39 42 30  1  5 58 38 52 21 41 36  9\n",
            " 61 59 24 31 25  2 43 63 44 55  7  6 46  3 27 10 45 26 18 15 20  4 56 16\n",
            " 34 60 14 22 62 19 51 12  8 37 48 33 53 50 13 35]\n",
            "Processing layer 2, original layer has 64 filters, pruning model has 48 filters\n",
            "torch.Size([64, 64, 3, 3])\n",
            "tensor([0.9568, 0.7575, 1.0117, 0.6820, 0.7745, 1.0443, 0.8286, 0.7457, 1.3126,\n",
            "        0.9962, 1.0340, 0.7761, 0.9243, 1.2169, 0.9924, 1.0401, 0.9995, 0.9957,\n",
            "        0.9023, 0.8100, 0.8217, 0.9245, 0.8151, 0.9718, 1.0651, 1.1251, 0.7104,\n",
            "        0.9538, 0.8629, 1.0188, 0.9096, 1.1071, 0.8531, 0.9000, 1.0658, 0.6484,\n",
            "        1.0872, 1.3863, 1.0284, 0.8155, 0.8064, 0.9070, 0.7479, 0.8785, 1.0194,\n",
            "        1.0796, 0.6774, 0.8222, 0.7790, 0.9399, 0.8873, 1.0353, 1.2583, 0.7162,\n",
            "        1.0076, 0.8251, 0.8711, 0.8425, 1.1224, 1.0227, 1.1827, 0.7698, 0.7703,\n",
            "        1.0230], device='cuda:0')\n",
            "rank [37  8 52 13 60 25 58 31 36 45 34 24  5 15 51 10 38 63 59 44 29  2 54 16\n",
            "  9 17 14 23  0 27 49 21 12 30 41 18 33 50 43 56 28 32 57  6 55 47 20 39\n",
            " 22 19 40 48 11  4 62 61  1 42  7 53 26  3 46 35]\n",
            "Processing layer 3, original layer has 128 filters, pruning model has 96 filters\n",
            "torch.Size([128, 64, 3, 3])\n",
            "tensor([0.9056, 1.2032, 0.8065, 1.0508, 0.6540, 0.8431, 0.8092, 1.1048, 0.6802,\n",
            "        0.6213, 0.7663, 1.0939, 0.6523, 0.8082, 1.0406, 1.1564, 0.7518, 0.8792,\n",
            "        0.8538, 0.6408, 0.9373, 0.9123, 0.5689, 0.9607, 0.7212, 0.8859, 0.7632,\n",
            "        1.0145, 0.9863, 1.0127, 0.7843, 0.8412, 0.9535, 0.6994, 0.8776, 0.9314,\n",
            "        0.8150, 0.9517, 1.0069, 0.7411, 1.0646, 0.6929, 0.9094, 0.9399, 0.8157,\n",
            "        1.0090, 1.0515, 0.7427, 0.9932, 0.7515, 0.9520, 0.8851, 0.8249, 0.8667,\n",
            "        0.9560, 0.8266, 1.2360, 0.8769, 0.8644, 0.7465, 1.0445, 0.6287, 0.9047,\n",
            "        0.9438, 0.9550, 0.8328, 0.8971, 0.8171, 0.7848, 0.7398, 1.1947, 0.7917,\n",
            "        0.7963, 0.9090, 0.8877, 0.8802, 1.0609, 0.8857, 0.9349, 0.7314, 0.9118,\n",
            "        0.8175, 0.8522, 0.9819, 1.1633, 0.9387, 0.9145, 0.8825, 0.9989, 0.9490,\n",
            "        0.9121, 0.7317, 1.0114, 0.7144, 0.5084, 0.9593, 0.7101, 0.9798, 0.8486,\n",
            "        1.0480, 0.8610, 0.7708, 1.0563, 0.5679, 1.0435, 0.8778, 0.8651, 0.8893,\n",
            "        0.6839, 0.8352, 0.9592, 1.1133, 0.8513, 0.9218, 0.7519, 0.7551, 0.6435,\n",
            "        1.0919, 0.8421, 0.8340, 0.9382, 0.9341, 0.7823, 0.9830, 0.8949, 0.7779,\n",
            "        0.7818, 0.8259], device='cuda:0')\n",
            "rank [ 56   1  70  84  15 111   7  11 117  40  76 102  46   3  99  60 104  14\n",
            "  27  29  92  45  38  88  48  28 123  83  97  23  95 110  54  64  32  50\n",
            "  37  89  63  43  85 120  20  78 121  35 113  86  21  90  80  42  73   0\n",
            "  62  66 124 107  74  25  77  51  87  75  17 105  34  57  53 106  58 100\n",
            "  18  82 112  98   5 118  31 109 119  65  55 127  52  81  67  44  36   6\n",
            "  13   2  72  71  68  30 122 126 125 101  10  26 115 114  16  49  59  47\n",
            "  39  69  91  79  24  93  96  33  41 108   8   4  12 116  19  61   9  22\n",
            " 103  94]\n",
            "Processing layer 4, original layer has 128 filters, pruning model has 96 filters\n",
            "torch.Size([128, 128, 3, 3])\n",
            "tensor([1.0231, 1.0454, 1.2206, 1.0917, 1.2518, 1.1505, 1.1474, 1.0384, 1.0518,\n",
            "        0.9048, 0.9241, 1.0116, 1.0843, 1.2857, 1.1812, 1.2578, 1.1087, 1.1351,\n",
            "        0.9463, 1.0858, 0.9617, 0.9937, 0.9517, 0.8922, 1.1173, 1.1034, 1.0607,\n",
            "        1.1873, 1.0445, 1.0623, 1.1709, 1.0459, 0.9131, 1.0693, 0.9923, 1.0882,\n",
            "        1.0070, 1.1053, 1.1287, 1.1162, 1.1155, 1.0448, 0.9768, 0.9106, 1.1081,\n",
            "        1.2872, 1.0263, 0.9726, 1.1461, 1.0607, 1.0892, 1.1698, 0.9422, 1.0944,\n",
            "        1.1779, 0.9242, 0.8192, 0.9849, 1.0097, 1.0721, 0.9904, 1.0608, 1.0419,\n",
            "        0.7969, 1.0923, 0.9972, 1.1887, 0.9242, 1.0014, 0.9234, 0.8947, 1.0463,\n",
            "        1.1040, 1.1585, 1.0215, 1.1115, 1.1072, 0.9435, 1.0690, 0.9920, 1.1128,\n",
            "        0.9135, 0.9809, 1.0261, 0.7960, 1.1044, 1.0139, 0.8884, 1.1172, 1.0516,\n",
            "        0.9758, 1.0344, 1.0033, 1.0105, 1.0380, 1.1240, 1.0154, 1.0536, 0.9179,\n",
            "        1.1002, 1.0663, 1.0195, 0.9894, 1.0369, 1.0963, 0.9281, 1.1952, 1.2703,\n",
            "        1.0427, 0.9608, 1.2018, 0.9992, 0.9976, 1.0670, 1.0453, 0.9500, 1.0411,\n",
            "        1.1848, 0.9663, 1.0476, 1.0834, 1.2515, 1.2724, 1.0816, 1.0633, 0.9146,\n",
            "        1.2131, 1.0363], device='cuda:0')\n",
            "rank [ 45  13 122 107  15   4 121   2 126 110 106  66  27 117  14  54  30  51\n",
            "  73   5   6  48  17  38  95  24  88  39  40  80  75  16  44  76  37  85\n",
            "  72  25  99 104  53  64   3  50  35  19  12 120 123  59  33  78 113 100\n",
            " 124  29  61  49  26  97   8  89 119  71  31   1 114  41  28 108  62 116\n",
            "   7  94 103 127  91  46  83   0  74 101  96  86  11  93  58  36  92  68\n",
            " 111 112  65  21  34  79  60 102  57  82  42  90  47 118  20 109  22 115\n",
            "  18  77  52 105  55  67  10  69  98 125  81  32  43   9  70  23  87  56\n",
            "  63  84]\n",
            "Processing layer 5, original layer has 256 filters, pruning model has 192 filters\n",
            "torch.Size([256, 128, 3, 3])\n",
            "tensor([0.8461, 0.9232, 0.9796, 1.1330, 0.9846, 0.9890, 0.9906, 0.8667, 0.8769,\n",
            "        1.0311, 1.0158, 0.7897, 0.8149, 0.9340, 1.0628, 1.0023, 1.0149, 0.9040,\n",
            "        0.8680, 0.9649, 0.9560, 0.8570, 0.9436, 0.7777, 0.8655, 0.8244, 1.0308,\n",
            "        0.8641, 0.9364, 0.9781, 0.8858, 0.7877, 0.9450, 1.0429, 0.9415, 1.0126,\n",
            "        0.9076, 1.0077, 0.9098, 0.8726, 0.9587, 1.0014, 0.9070, 0.7537, 1.0207,\n",
            "        1.0161, 0.9512, 0.7911, 0.8965, 0.9276, 0.8665, 0.8889, 0.8957, 0.9366,\n",
            "        1.0546, 0.7356, 1.0615, 0.8487, 0.7670, 0.8657, 1.0829, 0.8350, 0.9501,\n",
            "        0.8192, 0.9940, 1.0186, 1.0370, 0.9247, 0.8490, 0.9787, 0.8965, 0.8848,\n",
            "        0.8765, 0.9511, 0.9411, 0.9688, 0.8671, 0.9115, 0.9581, 0.9502, 0.9638,\n",
            "        0.9066, 0.8960, 0.9372, 0.9483, 0.9139, 1.0482, 1.0611, 0.9499, 0.7736,\n",
            "        0.9059, 0.9403, 0.9167, 1.0857, 0.8858, 0.9932, 0.8763, 0.8811, 0.9114,\n",
            "        0.8832, 0.9335, 1.1627, 0.8005, 0.9833, 0.8880, 0.9150, 0.9251, 0.9277,\n",
            "        0.7593, 0.9075, 0.8598, 0.9795, 1.0132, 0.8847, 0.7912, 0.9590, 0.8568,\n",
            "        1.1233, 0.9601, 0.9357, 0.8982, 0.9408, 0.8780, 0.9030, 0.9698, 1.0651,\n",
            "        1.0717, 0.9525, 0.9857, 0.9649, 0.9884, 0.9605, 0.9853, 1.0957, 0.8976,\n",
            "        0.8067, 0.9314, 1.1078, 0.9898, 0.8442, 1.0309, 0.9896, 1.0526, 0.9674,\n",
            "        0.7821, 0.8983, 0.9930, 1.0320, 0.8825, 0.9546, 0.9959, 0.9414, 0.8891,\n",
            "        1.0347, 1.0175, 0.8817, 0.9771, 0.8960, 0.9602, 0.9515, 0.8791, 0.9918,\n",
            "        1.0068, 0.9165, 0.9175, 0.9338, 0.9524, 0.8509, 0.8345, 1.0035, 0.8920,\n",
            "        1.0554, 0.9472, 0.8728, 0.8593, 0.9629, 0.8994, 0.8529, 0.9275, 0.9571,\n",
            "        0.8996, 0.9324, 0.9050, 0.8007, 0.8998, 0.9872, 0.9896, 0.8734, 0.9505,\n",
            "        0.9816, 0.9393, 1.0159, 0.9099, 0.9813, 0.8866, 0.8663, 0.9328, 0.9581,\n",
            "        1.0526, 0.9851, 0.9504, 0.8333, 0.9199, 0.8959, 0.9159, 0.8936, 0.9308,\n",
            "        0.8811, 0.9378, 0.9415, 1.0588, 0.9794, 0.9934, 0.8327, 0.7932, 0.9004,\n",
            "        0.8929, 0.9057, 1.1133, 0.8710, 1.0030, 1.0492, 1.0250, 1.0269, 0.8406,\n",
            "        0.9501, 1.0673, 0.9295, 0.9956, 1.0263, 0.9736, 0.8417, 0.8730, 0.8783,\n",
            "        0.8618, 1.0596, 0.8860, 1.0788, 1.0146, 0.8570, 0.9566, 0.9563, 0.9213,\n",
            "        0.8256, 0.8886, 0.8906, 0.9696, 0.8690, 0.7900, 0.9356, 0.9320, 0.9529,\n",
            "        0.8635, 0.8925, 0.6859, 0.9850], device='cuda:0')\n",
            "rank [101   3 117 218 137 133  93  60 237 126 226 125  14  56  87 235 210 171\n",
            "  54 198 142 221  86  33  66 153 147   9 140  26 223 229 222  44  65 154\n",
            "  45 191  10  16 238 112  35  37 162 169 220  15  41 150 228  64 212  95\n",
            " 146 161   6 138 141 186   5 130 185 128 132 199 255   4 103 189 193   2\n",
            " 111 211  69  29 156 230 124 246  75 143 129  19  80 175 131 158 118 115\n",
            "  40  78 197 179 240 241  20 149 251 127 166 159  46  73 188 200  79 225\n",
            "  62  88  84 172  32  22  34 209 151  74 121  91 190 208  83  53  28 119\n",
            " 249  13 165 100 196 181 250 136 206 227 107  49 178 106  67   1 242 202\n",
            " 164  92 163 204 105  85  77  98 192  38  36 109  42  81  90 217 182  17\n",
            " 123 215 184 180 176 145 120 134  70  48  82 157 203  52 205 216 253 170\n",
            " 245 152  51 244 104 194 236  94  30  71 113  99 148 155  97 207 160 233\n",
            " 122   8  72  96 187 232 173  39 219 247  18  76   7  50 195  59  24  27\n",
            " 252 234 110 174  21 239 116 177 167  68  57   0 139 231 224  61 168 201\n",
            " 213 243  25  63  12 135 183 102 214 114  47 248  11  31 144  23  89  58\n",
            " 108  43  55 254]\n",
            "Processing layer 6, original layer has 256 filters, pruning model has 192 filters\n",
            "torch.Size([256, 256, 3, 3])\n",
            "tensor([1.1393, 0.9696, 0.9510, 0.9116, 1.0520, 0.8667, 0.9868, 1.1479, 0.7644,\n",
            "        1.0532, 1.1246, 1.0023, 1.0956, 1.0918, 0.9615, 0.8127, 1.0691, 1.0374,\n",
            "        0.9313, 1.1300, 0.8395, 0.8365, 1.0507, 0.9030, 0.9064, 1.0578, 0.7801,\n",
            "        1.0096, 0.8538, 1.1821, 1.0081, 1.0181, 0.9270, 1.0705, 0.9107, 0.9102,\n",
            "        1.0551, 1.1923, 0.8286, 0.8251, 0.9371, 1.0202, 0.9889, 0.9843, 0.9155,\n",
            "        1.1469, 1.1198, 0.8400, 1.0151, 0.8569, 1.0574, 1.1069, 0.9810, 0.9440,\n",
            "        0.8464, 1.0430, 1.0848, 1.3528, 1.1154, 1.1277, 1.1040, 0.9503, 1.1370,\n",
            "        1.0350, 0.8754, 1.0116, 0.7670, 0.6487, 0.9960, 0.9508, 0.9538, 0.8562,\n",
            "        0.8853, 0.8682, 0.8843, 1.0566, 0.9573, 0.9023, 1.0475, 0.8780, 1.0285,\n",
            "        0.9444, 0.9884, 1.0163, 0.9612, 0.8453, 1.0123, 0.8693, 0.9603, 1.0276,\n",
            "        0.9697, 1.0618, 1.0649, 1.0517, 0.8783, 1.0818, 0.9609, 0.8736, 0.9446,\n",
            "        0.9675, 0.9834, 0.9508, 0.9670, 0.9143, 1.0972, 1.0355, 0.9813, 0.8556,\n",
            "        0.9035, 0.8944, 1.0551, 1.1224, 0.9203, 1.0111, 0.8077, 0.9693, 1.0669,\n",
            "        0.9601, 0.9472, 1.0454, 0.9045, 1.1342, 1.1666, 0.9442, 1.0338, 0.8765,\n",
            "        0.9417, 1.0989, 0.9517, 0.8746, 0.8425, 0.9688, 0.9338, 1.0517, 1.0503,\n",
            "        1.0142, 0.9762, 1.1257, 0.8447, 1.0155, 1.1394, 1.0343, 0.8689, 1.0478,\n",
            "        1.1028, 0.8719, 0.7800, 0.9544, 0.9602, 1.1345, 1.0896, 0.9747, 0.9461,\n",
            "        1.0065, 1.2760, 0.9591, 0.9925, 0.8729, 0.9115, 1.0381, 0.9585, 0.9471,\n",
            "        0.9919, 0.9065, 1.1755, 1.0534, 0.9864, 1.0049, 1.0526, 0.8026, 0.9390,\n",
            "        1.0453, 0.9771, 1.1384, 0.9309, 1.0480, 0.9945, 1.0393, 0.9875, 1.0261,\n",
            "        0.8767, 1.0289, 0.8802, 0.9386, 1.0452, 1.0041, 0.7416, 1.0413, 1.0704,\n",
            "        0.9601, 0.9465, 0.9273, 0.9905, 0.8516, 1.0013, 0.8359, 0.9831, 1.0658,\n",
            "        1.0772, 1.0348, 0.8670, 1.0601, 0.9204, 1.1269, 0.9374, 0.9424, 0.8664,\n",
            "        0.9996, 1.0622, 1.2281, 0.8689, 0.9494, 0.9564, 1.0885, 0.9561, 0.8894,\n",
            "        0.9992, 1.0544, 0.9667, 1.0579, 1.0753, 0.8832, 1.0229, 1.1603, 1.0246,\n",
            "        0.9265, 0.9192, 0.9569, 0.8528, 0.9310, 0.9918, 0.8742, 0.9164, 0.8442,\n",
            "        1.0951, 0.8695, 1.0501, 1.0618, 1.0269, 1.0077, 1.0783, 1.0561, 0.9661,\n",
            "        1.0579, 0.7767, 1.0725, 0.9627, 1.0964, 0.9178, 0.9932, 0.7609, 0.9281,\n",
            "        0.9790, 0.8548, 0.8784, 0.9103], device='cuda:0')\n",
            "rank [ 57 154 209  37  29 164 122 223   7  45 140   0 173  62 149 121  19  59\n",
            " 203 137  10 111  46  58  51  60 144 127 104 247  12 234  13 150 213  56\n",
            "  95 240 198 220 245  33 188  16 116 197  92 208  91 237 201 243 219  25\n",
            "  50  75 241  36 110 217 165   9 168   4 133  93  22 134 236 175 143  78\n",
            " 119 171 184  55 187 177 159  17 105  63 199 141 124 181  80  89 238 179\n",
            " 224 222  41  31  83 139  48 135  86  65 113  27  30 239 153 167 185  11\n",
            " 194 207 216  68 176 249 156 162 230 192  42  82 178   6 166  43 100 196\n",
            " 106  52 252 172 136 151  90   1 115 131  99 102 218 242 246  14  84  96\n",
            "  88 148 117 189 155 160  76 227 212 214 147  70 128   2  69 101  61 211\n",
            " 118 161 190 152  98  81 123  53 205 126 170 183 204  40 132  18 229 174\n",
            " 251 191  32 225 202 112 226 248 232  44 103   3 158  34 255  35 163  24\n",
            " 120 108  23  77 109 215  72  74 221 182 254  94  79 180 125  64 129 231\n",
            "  97 157 145 235  87 142 210  73 200   5 206  49  71 107 253  28 228 193\n",
            "  54  85 138 233 130  47  20  21 195  38  39  15 114 169  26 146 244  66\n",
            "   8 250 186  67]\n",
            "Processing layer 7, original layer has 256 filters, pruning model has 192 filters\n",
            "torch.Size([256, 256, 3, 3])\n",
            "tensor([0.7839, 0.8768, 0.8562, 0.9486, 0.9061, 0.7997, 0.8143, 0.8145, 0.9756,\n",
            "        1.0431, 0.9903, 0.8046, 0.7741, 0.9971, 0.8233, 0.9227, 0.9413, 1.0159,\n",
            "        0.9788, 0.9140, 0.8190, 0.8595, 0.7210, 0.8175, 0.8243, 0.9547, 0.8243,\n",
            "        1.0325, 0.8497, 1.0122, 0.8960, 0.9452, 0.9197, 0.8480, 1.0316, 0.7774,\n",
            "        0.9913, 0.8952, 0.9929, 0.9208, 0.9992, 0.7717, 0.8859, 0.8514, 0.8318,\n",
            "        0.9066, 0.9071, 0.8845, 0.7249, 0.7035, 0.8327, 0.6884, 0.8355, 0.6832,\n",
            "        0.9035, 0.8975, 0.8683, 0.7731, 0.9233, 0.8603, 1.0390, 0.9802, 0.9308,\n",
            "        0.7633, 1.0618, 0.7983, 0.9149, 0.7910, 0.8376, 1.0066, 0.9383, 0.9532,\n",
            "        1.0268, 0.8929, 0.8017, 0.9182, 0.7442, 1.0297, 0.9136, 0.8814, 0.8931,\n",
            "        0.8609, 0.9256, 0.8824, 0.8213, 0.9019, 0.8187, 0.9203, 0.9524, 1.0831,\n",
            "        0.7759, 0.9059, 0.7137, 0.9950, 0.9618, 0.7506, 0.9667, 0.9181, 0.8219,\n",
            "        0.6471, 0.8669, 0.8859, 0.8240, 0.8752, 0.7824, 0.9758, 0.9872, 0.8351,\n",
            "        0.9336, 0.9379, 0.8962, 1.1295, 0.9657, 0.9086, 0.8770, 0.7431, 0.8882,\n",
            "        0.8920, 0.8541, 0.9433, 0.8165, 1.0099, 0.8478, 0.9050, 0.9038, 0.9046,\n",
            "        0.8334, 1.0541, 1.0066, 0.7870, 1.0096, 0.9163, 0.9658, 1.0967, 0.8316,\n",
            "        0.7749, 0.7828, 0.8531, 0.8589, 0.9624, 0.8473, 0.8773, 0.7472, 0.9231,\n",
            "        0.8641, 0.9830, 0.8581, 0.8695, 1.0000, 0.7478, 0.9378, 0.7647, 0.8377,\n",
            "        0.8599, 0.8179, 0.9144, 0.9026, 0.8772, 0.8479, 0.9365, 0.7205, 1.0061,\n",
            "        0.9447, 0.8891, 0.8639, 0.9314, 0.9675, 0.9481, 0.6345, 0.9697, 0.8793,\n",
            "        0.9940, 0.8485, 1.0168, 0.9258, 0.9469, 0.9591, 0.9906, 1.0447, 0.7510,\n",
            "        0.9471, 0.9715, 0.9519, 0.9582, 0.9950, 0.8914, 0.8584, 0.9073, 0.8666,\n",
            "        0.8748, 0.8694, 0.8852, 0.8446, 0.8069, 0.7229, 0.7869, 0.9028, 0.8037,\n",
            "        0.8726, 0.8735, 0.8469, 0.9880, 0.9709, 0.8820, 0.8146, 0.8738, 0.7977,\n",
            "        0.9326, 0.9061, 0.8691, 0.8462, 0.7919, 1.0603, 0.9905, 0.9209, 0.8138,\n",
            "        1.0399, 0.8857, 1.0841, 1.0968, 0.9400, 0.8941, 0.9675, 0.7573, 0.9557,\n",
            "        0.8363, 0.7843, 0.8476, 0.8529, 0.8605, 0.8286, 1.0321, 0.8462, 1.0932,\n",
            "        1.0173, 0.9147, 0.8533, 1.0235, 1.0716, 0.8273, 0.8493, 0.9830, 0.9971,\n",
            "        0.9383, 0.9026, 0.8821, 0.9392, 0.9522, 0.8409, 0.9194, 0.7814, 0.9909,\n",
            "        0.8878, 0.9710, 0.8551, 0.7729], device='cuda:0')\n",
            "rank [111 219 133 233 218  89 238  64 212 127 178   9 216  60  27 231  34  77\n",
            "  72 237 234 173  17  29 121 130 128  69 161 148  40  13 242  93 184 171\n",
            "  38  36 251 177 213  10 201 106 241 145  61  18 105   8 181 253 202 169\n",
            " 222 166  96 132 112 139  94 176 183 224  25  71  88 247 182   3 167 180\n",
            " 175  31 162 119  16 220 246 243  70 109 150 159 108 207 165  62 174  82\n",
            "  58 143  15 214  39  87  32 249  75  97 131  66 235 155  19  78 113 187\n",
            "  46  45   4 208  91 123 125 124  54 196 244 156  85  55 110  30  37 221\n",
            "  80  73 117 185 163 116 252 101  42 217 191  47  83 245 203  79 170 141\n",
            " 157 114   1 103 189 205 199 198 147 190 209  56 100 188 144 164  81 229\n",
            "  59 153  21 138 186 146   2 254 118 236 137 228  43  28 240 172  33 158\n",
            " 122 227 140 200 232 210 192 248 152  68 225  52 107 126  50  44 134 230\n",
            " 239  24  26 102  14  98  84  20  86 154  23 120 204   7   6 215 193  11\n",
            " 197  74   5  65 206 211  67 129 195 226   0 136 104 250  35  90 135  12\n",
            "  57 255  41 151  63 223 179  95 149 142  76 115  48 194  22 160  92  49\n",
            "  51  53  99 168]\n",
            "Processing layer 8, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 256, 3, 3])\n",
            "tensor([0.3924, 0.6133, 0.3899, 0.9467, 0.4961, 0.5762, 0.5570, 0.6759, 0.7545,\n",
            "        0.4806, 0.3990, 0.6177, 0.4761, 0.6208, 0.4477, 0.5866, 0.8410, 0.4024,\n",
            "        0.4413, 0.4484, 0.4107, 0.4374, 1.1125, 0.4102, 0.6466, 0.4517, 0.4721,\n",
            "        0.5022, 0.5002, 0.5039, 0.5700, 0.5343, 0.5459, 0.5483, 0.6681, 0.6981,\n",
            "        0.6710, 0.3195, 0.6760, 0.4015, 0.4123, 0.5274, 0.3886, 0.4656, 0.5218,\n",
            "        0.2372, 0.3832, 0.5567, 0.5187, 0.5661, 0.7076, 0.4699, 0.6495, 0.3480,\n",
            "        0.3431, 0.4385, 0.2167, 0.3901, 0.5099, 0.5983, 0.3871, 0.4936, 0.6564,\n",
            "        0.6305, 0.4258, 0.7851, 0.5828, 0.3116, 0.4569, 0.3722, 0.5134, 0.4672,\n",
            "        0.4845, 0.5122, 0.5736, 0.5456, 0.5067, 0.4668, 0.4579, 0.5934, 0.3571,\n",
            "        0.4708, 0.5206, 0.4440, 0.6514, 0.3599, 0.4914, 0.7972, 0.4163, 0.4504,\n",
            "        0.6568, 0.3539, 0.5716, 0.4440, 0.4010, 0.5316, 0.6618, 0.4469, 0.5581,\n",
            "        0.3736, 0.4793, 0.4766, 0.6713, 0.5557, 0.3356, 0.3927, 0.3674, 0.4242,\n",
            "        0.7505, 0.5591, 0.5040, 0.4323, 0.7897, 0.5425, 0.5993, 0.4331, 0.4932,\n",
            "        0.3885, 0.6136, 0.3901, 0.3425, 0.6444, 0.4432, 0.4673, 0.6620, 0.3593,\n",
            "        0.4477, 0.4857, 0.3903, 0.6651, 0.4373, 0.5386, 0.5597, 0.5889, 0.5413,\n",
            "        0.4326, 0.3421, 0.4676, 0.4335, 0.5567, 0.5553, 0.6559, 0.5152, 0.4001,\n",
            "        0.3121, 0.4356, 0.6130, 0.6450, 0.5306, 0.3481, 0.5224, 0.4614, 0.5384,\n",
            "        0.6991, 0.5658, 0.5401, 0.4712, 0.4820, 0.4003, 0.3808, 0.4720, 0.6524,\n",
            "        0.4218, 0.5804, 0.4490, 0.4951, 0.5194, 0.6838, 0.4814, 0.7650, 0.6185,\n",
            "        0.4836, 0.5801, 0.6259, 0.4990, 0.8079, 0.4546, 0.3882, 0.6490, 0.5178,\n",
            "        0.6226, 0.4189, 0.4247, 0.4634, 0.5088, 0.3730, 0.7114, 0.3933, 0.4754,\n",
            "        0.7543, 0.4116, 0.4676, 0.4522, 0.5826, 0.6575, 0.4700, 0.5384, 0.5927,\n",
            "        0.4495, 0.6460, 0.4964, 0.9203, 0.6935, 0.2664, 0.5852, 0.7320, 0.5053,\n",
            "        0.5384, 0.4537, 0.4157, 0.4218, 0.3371, 0.6926, 0.7835, 0.5297, 0.4602,\n",
            "        0.3605, 0.5295, 0.4991, 0.4116, 0.4285, 0.4131, 0.3683, 0.7295, 0.6597,\n",
            "        0.3345, 0.4370, 0.5828, 0.6093, 0.5777, 0.6341, 0.5452, 0.4250, 0.3497,\n",
            "        0.5684, 0.7408, 0.3753, 0.5261, 0.5595, 0.4400, 0.4328, 0.4098, 0.5739,\n",
            "        0.7755, 0.4901, 0.5687, 0.4942, 0.4371, 0.5660, 0.5116, 0.5864, 0.5540,\n",
            "        0.6307, 0.3327, 0.6252, 0.3648, 0.4792, 0.5011, 0.6262, 0.6844, 0.5583,\n",
            "        0.5370, 0.4653, 0.5789, 0.6148, 0.5938, 0.4343, 0.6003, 0.3795, 0.3240,\n",
            "        0.4162, 0.7324, 0.2632, 0.8069, 0.5971, 0.6302, 0.4618, 0.4443, 0.6695,\n",
            "        0.5260, 0.6282, 0.3692, 0.7309, 0.4783, 0.4417, 0.6401, 0.5716, 0.3153,\n",
            "        0.3465, 0.4300, 0.7412, 0.7173, 0.6073, 0.4054, 0.6345, 0.5662, 0.3262,\n",
            "        0.5073, 0.2813, 0.7256, 0.5212, 0.5372, 0.5561, 0.5409, 0.6336, 0.4586,\n",
            "        0.6366, 0.5033, 0.5094, 0.6218, 0.5177, 0.6444, 0.4122, 0.4210, 0.7903,\n",
            "        0.7012, 0.5341, 0.4039, 0.4175, 0.5128, 0.5025, 0.7674, 0.5655, 0.5434,\n",
            "        0.5908, 0.5800, 0.4712, 0.3333, 0.2910, 0.4928, 0.3398, 0.5611, 0.5851,\n",
            "        0.4421, 0.5179, 0.4087, 0.5445, 0.6571, 0.3538, 0.5895, 0.5962, 0.5476,\n",
            "        0.6136, 0.7130, 0.3622, 0.4251, 0.4558, 0.5062, 0.4828, 0.3380, 0.6538,\n",
            "        0.3836, 0.6527, 0.6342, 0.4643, 0.6125, 0.5419, 0.5220, 0.4694, 0.6890,\n",
            "        0.6008, 0.5018, 0.5797, 0.3552, 0.5235, 0.4839, 0.4321, 0.2481, 0.6313,\n",
            "        0.5043, 0.5482, 0.5052, 0.5337, 0.4572, 0.5084, 0.2658, 0.5275, 0.5335,\n",
            "        0.4931, 0.4757, 0.4492, 0.4125, 0.5065, 0.4437, 0.5724, 0.6154, 0.5792,\n",
            "        0.3372, 0.5136, 0.6033, 0.2804, 0.5933, 0.5751, 0.5293, 0.4818, 0.6502,\n",
            "        0.3424, 0.5428, 0.5031, 0.3929, 0.3607, 0.4360, 0.6102, 0.5603, 0.7173,\n",
            "        0.5240, 0.6039, 0.6245, 0.4757, 0.4523, 0.5882, 0.4410, 0.3977, 0.7261,\n",
            "        0.5695, 0.4955, 0.5977, 0.4178, 0.5565, 0.3571, 0.4837, 0.3441, 0.3009,\n",
            "        0.4668, 0.3464, 0.3586, 0.5385, 0.7875, 0.6152, 0.4557, 0.7054, 0.4251,\n",
            "        0.5406, 0.3099, 0.4323, 0.4293, 0.3430, 0.3205, 0.6419, 0.5169, 0.6118,\n",
            "        0.5561, 0.3918, 0.5652, 0.5192, 0.4280, 0.4678, 0.5639, 0.5742, 0.5660,\n",
            "        0.4750, 0.3989, 0.3431, 0.5194, 0.7609, 0.3704, 0.6986, 0.4020, 0.2842,\n",
            "        0.5286, 0.5041, 0.6232, 0.4270, 0.5367, 0.6719, 0.4966, 0.7415, 0.3840,\n",
            "        0.5466, 0.3884, 0.4882, 0.5128, 0.4682, 0.5324, 0.5345, 0.6196, 0.3551,\n",
            "        0.5294, 0.4088, 0.5796, 0.7244, 0.4406, 0.5457, 0.5045, 0.6027, 0.6109,\n",
            "        0.5069, 0.3619, 0.5287, 0.8189, 0.6256, 0.6059, 0.5192, 0.6831, 0.3190,\n",
            "        0.8204, 0.4033, 0.5597, 0.4359, 0.4886, 0.5763, 0.5572, 0.6171, 0.5528,\n",
            "        0.6936, 0.2698, 0.4502, 0.4139, 0.4815, 0.5012, 0.5588, 0.5208],\n",
            "       device='cuda:0')\n",
            "rank [ 22   3 201  16 495 489 175 273  87 314 112 427  65 213 243 321 169 454\n",
            "   8 189 108 466 290 235 271 205 282 223 413 299 480 291 404 343 186  50\n",
            " 430 315 153 456  35 504 202 212 359 259 167 493  38   7 464 102  36 278\n",
            "  34 129 124  96 224 194 337  90  62 141 350 352 161  84 395  52 178  24\n",
            " 199 147 311 121 438 285 306 294 353 230 304 368 252  63 275 280 258 173\n",
            " 490 254 407 461 180 309  13 475 170  11 502 385 428 264 342 118   1 146\n",
            " 355 440 485 402 228 292 491 406 389 484 360 267 114  59 416 274 340 265\n",
            "  79 391 197 324 339 133 410  15 250 204 332 227  66 193 163 172 325 362\n",
            " 479 386 263 229 500   5 392 448 242  74 384  92 286  30 414 245 234 295\n",
            "  49 248 449 154 322 443 447 331 403 132 497 238 109 510 260  98 501   6\n",
            " 139  47 418 302 441 103 140 251 503  33 370 341 468  32 482  75 231 336\n",
            " 323 397 113 356 134 303 432 155 131 426 207 152 196 301 261 463 474  31\n",
            " 316 372 377 473  95 148 214 217 477 393 488 459 376  41 237 279 405 364\n",
            " 150 357  44 300 511  82 166 453 492 444  48 334 179 310 439 142 388  70\n",
            " 319 471  73 249  58 308 184 374 297 486  76 382 347 206 371 483 369 460\n",
            " 110  29 307 398 320  27 361 509 257  28 218 174 465 200   4 415 165 246\n",
            "  61 116 378 329  86 244 499 470 127  72 365 420 171 348 157 394 508 168\n",
            "   9 100 256 283 101  12 408 379 188 450  26 160 326 156  81 195  51 358\n",
            " 472 446 137 191 123  71 423  77  43 262 354 183 276 151 215 305  78 373\n",
            "  68 346 429 176 208 409 192  25  89 506 198 380 164  19  14 126  97 277\n",
            "  93  83 383 122 333 284  18 411 481 239  55  21 130 247 226 401 498 145\n",
            " 266 138 115 240 135 434 111 366 289 435 220 445 462  64 431 345 232 182\n",
            " 107 162 210 313 181 417 318  88 270 209 507 221 381  40 312 219 190  20\n",
            "  23 241 478 335 293 317 496  17 457  39  94 158 143  10 451 412 187 399\n",
            " 105   0 442 128  57 119   2  42 117 469 177  60 467 351  46 159 268 236\n",
            "  99 185  69 455 281 222 106 255 344 487 400 216  85 125 425 419  80 363\n",
            " 476  91 338 233 149  53 288 424 421  54 452 436 120 396 136 330 349 387\n",
            " 211 104 225 327 253 296 269 437  37 494 287 144  67 433 422 328 458 298\n",
            " 390 505 203 375 272 367  45  56]\n",
            "Processing layer 9, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.3400, 0.3731, 0.2459, 0.2907, 0.5719, 0.1646, 0.2407, 0.3343, 0.2825,\n",
            "        0.4461, 0.4344, 0.4081, 0.2097, 0.3693, 0.3085, 0.1512, 0.2547, 0.4675,\n",
            "        0.2605, 0.4644, 0.4318, 0.4732, 0.3245, 0.4159, 0.2407, 0.2925, 0.1827,\n",
            "        0.2949, 0.2251, 0.3583, 0.3995, 0.4264, 0.3285, 0.2499, 0.2370, 0.2478,\n",
            "        0.2691, 0.3596, 0.3553, 0.3680, 0.2456, 0.2967, 0.3415, 0.2241, 0.2506,\n",
            "        0.2622, 0.3264, 0.3920, 0.2327, 0.2689, 0.2618, 0.4584, 0.3417, 0.3433,\n",
            "        0.3565, 0.3050, 0.2296, 0.2493, 0.2967, 0.2265, 0.5685, 0.1965, 0.2792,\n",
            "        0.3499, 0.3358, 0.3238, 0.3665, 0.3009, 0.2390, 0.2480, 0.3193, 0.3732,\n",
            "        0.2667, 0.4316, 0.2145, 0.2115, 0.4031, 0.2972, 0.2987, 0.4437, 0.3188,\n",
            "        0.2943, 0.2752, 0.2920, 0.2784, 0.2351, 0.3840, 0.4324, 0.3354, 0.3488,\n",
            "        0.3367, 0.4669, 0.2559, 0.3480, 0.2603, 0.2919, 0.2094, 0.2071, 0.2595,\n",
            "        0.3414, 0.2220, 0.3379, 0.2774, 0.3659, 0.2882, 0.4035, 0.2776, 0.3435,\n",
            "        0.2911, 0.1778, 0.4086, 0.2823, 0.3428, 0.2871, 0.2928, 0.3820, 0.3467,\n",
            "        0.3262, 0.2657, 0.3130, 0.1607, 0.3842, 0.3451, 0.2667, 0.4301, 0.2760,\n",
            "        0.1702, 0.3240, 0.4059, 0.3450, 0.2451, 0.2912, 0.2979, 0.4333, 0.3159,\n",
            "        0.3132, 0.1858, 0.4955, 0.2672, 0.1867, 0.2695, 0.3737, 0.3523, 0.4612,\n",
            "        0.2010, 0.2559, 0.2461, 0.4458, 0.4148, 0.4862, 0.3349, 0.5248, 0.1756,\n",
            "        0.3670, 0.4731, 0.3068, 0.3851, 0.2545, 0.3982, 0.3331, 0.3275, 0.2651,\n",
            "        0.2924, 0.2937, 0.3405, 0.1584, 0.4673, 0.3650, 0.3080, 0.3926, 0.3980,\n",
            "        0.4747, 0.2183, 0.3136, 0.3446, 0.3244, 0.3204, 0.2309, 0.2281, 0.2370,\n",
            "        0.4134, 0.3403, 0.2981, 0.3239, 0.3285, 0.2340, 0.2893, 0.2982, 0.3163,\n",
            "        0.2002, 0.1924, 0.4086, 0.3631, 0.2908, 0.3413, 0.4170, 0.2501, 0.3761,\n",
            "        0.2488, 0.4285, 0.2381, 0.3290, 0.3364, 0.4580, 0.2484, 0.4836, 0.3839,\n",
            "        0.2940, 0.3305, 0.2944, 0.2536, 0.2432, 0.3321, 0.3013, 0.2647, 0.2344,\n",
            "        0.2021, 0.2176, 0.2438, 0.2324, 0.2871, 0.3344, 0.3177, 0.3571, 0.2198,\n",
            "        0.3426, 0.3734, 0.3312, 0.3060, 0.2616, 0.4104, 0.3295, 0.2650, 0.2098,\n",
            "        0.3804, 0.2327, 0.2448, 0.2573, 0.2813, 0.4808, 0.2713, 0.3311, 0.2720,\n",
            "        0.1889, 0.2960, 0.2754, 0.2898, 0.3555, 0.5231, 0.2520, 0.4034, 0.3590,\n",
            "        0.4251, 0.2165, 0.4578, 0.3436, 0.3293, 0.4471, 0.2918, 0.3822, 0.3238,\n",
            "        0.1824, 0.2126, 0.3340, 0.3454, 0.2469, 0.1877, 0.2695, 0.2893, 0.3631,\n",
            "        0.2146, 0.3557, 0.2760, 0.2538, 0.3918, 0.2777, 0.2854, 0.3330, 0.1766,\n",
            "        0.9213, 0.2753, 0.2797, 0.3001, 0.2853, 0.2157, 0.2348, 0.2421, 0.3465,\n",
            "        0.2652, 0.3682, 0.2943, 0.5105, 0.1856, 0.3056, 0.2642, 0.3938, 0.3164,\n",
            "        0.4967, 0.3144, 0.4320, 0.3276, 0.2822, 0.2458, 0.2163, 0.2826, 0.3113,\n",
            "        0.4196, 0.2636, 0.4799, 0.3369, 0.3959, 0.4846, 0.1618, 0.3356, 0.5261,\n",
            "        0.2186, 0.3955, 0.2350, 0.2982, 0.4380, 0.1878, 0.2718, 0.1852, 0.3697,\n",
            "        0.3380, 0.3302, 0.3936, 0.3162, 0.2825, 0.4823, 0.3053, 0.2673, 0.3264,\n",
            "        0.5255, 0.4188, 0.3980, 0.2698, 0.4334, 0.2424, 0.2759, 0.3645, 0.2945,\n",
            "        0.4020, 0.2819, 0.3247, 0.3427, 0.2303, 0.2386, 0.3668, 0.2904, 0.3478,\n",
            "        0.2860, 0.2970, 0.2986, 0.2846, 0.4346, 0.2551, 0.2447, 0.2681, 0.3754,\n",
            "        0.2855, 0.4628, 0.3330, 0.2779, 0.4257, 0.2945, 0.2274, 0.4049, 0.4092,\n",
            "        0.2794, 0.3092, 0.3337, 0.4474, 0.3176, 0.3180, 0.2083, 0.2630, 0.3643,\n",
            "        0.1954, 0.2755, 0.3844, 0.3826, 0.1867, 0.2673, 0.3168, 0.3663, 0.3155,\n",
            "        0.3382, 0.3115, 0.3018, 0.2348, 0.2494, 0.2686, 0.2957, 0.2600, 0.3952,\n",
            "        0.3302, 0.4119, 0.1787, 0.2060, 0.3363, 0.3289, 0.2952, 0.2594, 0.2227,\n",
            "        0.2404, 0.1748, 0.2380, 0.3037, 0.1731, 0.3443, 0.2229, 0.3245, 0.3468,\n",
            "        0.3152, 0.4882, 0.2917, 0.2134, 0.4452, 0.3341, 0.3521, 0.2175, 0.3697,\n",
            "        0.3306, 0.4452, 0.4085, 0.4061, 0.2625, 0.2866, 0.3349, 0.1885, 0.2949,\n",
            "        0.3039, 0.2525, 0.4577, 0.3411, 0.2740, 0.3485, 0.2006, 0.1808, 0.3532,\n",
            "        0.2170, 0.3263, 0.3167, 0.3429, 0.2150, 0.3194, 0.2245, 0.2814, 0.2284,\n",
            "        0.2686, 0.3235, 0.3632, 0.3201, 0.1997, 0.3735, 0.2798, 0.2944, 0.2715,\n",
            "        0.2959, 0.2149, 0.3376, 0.2061, 0.3205, 0.2686, 0.3380, 0.4320, 0.2589,\n",
            "        0.2269, 0.2260, 0.6617, 0.3783, 0.3364, 0.3866, 0.2625, 0.1359, 0.4873,\n",
            "        0.4171, 0.2479, 0.1733, 0.4227, 0.5040, 0.2475, 0.3179, 0.3668, 0.2731,\n",
            "        0.2481, 0.3234, 0.4601, 0.4307, 0.2529, 0.3211, 0.3854, 0.2097, 0.1969,\n",
            "        0.3455, 0.2639, 0.3263, 0.5125, 0.4441, 0.2191, 0.2948, 0.2527, 0.2069,\n",
            "        0.3548, 0.2625, 0.3769, 0.3121, 0.2988, 0.2339, 0.2636, 0.2225],\n",
            "       device='cuda:0')\n",
            "rank [279 470   4  60 314 333 151 248 498 291 481 297 137 415 476 149 311 205\n",
            " 329 239 308 171  21 154  17 166  91  19 361 143 488  51 203 254 434 372\n",
            " 257   9 147 424 418 499  79 319 355  10 337 133  87 466 299  20  73 489\n",
            " 124 199  31 364 252 480 306 334 477 195  23 148 180 397 230 368 191 110\n",
            " 425  11 426 128 367 105 250  76 342  30 158 170 335 310 316 395 295 326\n",
            " 169  47 274 473 492 156 380 121  86 206 381 259 115 234 471 506 197 359\n",
            " 141 455 226  71   1 422 323  13 289  39 153 484 348  66 385 103 167 340\n",
            " 377 452 192 269  37 251  29 223  54 271 247  38 504 440 142 420  63  89\n",
            " 437  93 350 413 116 287 495 264 122 129 174 410 255 107  53 444 112 345\n",
            " 225  52  42  99 194 435 164 181   0 387 465 324 101 461 309  90 202 472\n",
            " 400  64 313  88 429 150 221   7 419 263 371 159 277 362 212 227 241 423\n",
            " 208 325 396 231 256 201 401  32 184 300 160  46 332 442 497 117 344  22\n",
            " 412 175 127 183  65 260 451 487 491 463 176 453 446  70  80 374 483 222\n",
            " 373 384 443 296 188 327 134 386 414 298 173 135 119 507 388 305 370  14\n",
            " 168 155 228 293 330  55 432 408 389 213  67 282 508  78 353 318 187 182\n",
            " 132  77 352  58  41 244 459 393 402 431  27 501 365 341 457 209 290  81\n",
            " 207 163 114  25 162  83  95 258 416 131 108 193   3 349 246 186 268 104\n",
            " 113 220 428 351 360 276 283 354 304 328   8 111 301 343 448 238 456 281\n",
            " 369  62  84 363 275 106 102 272 125 339 379 245 280  82 436 485 242 321\n",
            " 458 240 336 267 140  36  49 450 464 392 358 383 331 138 123  72 118 288\n",
            " 161 232 214 294 496 307 510 376 474 505 427  45  50 229  18  94 394  98\n",
            " 403 467 237 145  92 356  16 157 273 210 490 502 433 249  44 196  33 391\n",
            "  57 198 204 486  69 478  35 482 265 146   2 302  40 130 236 357 218 211\n",
            " 338 286  24   6 405  68 347 200 407 179  34  85 317 390 285 215 185 509\n",
            "  48 235 219 177 346  56 449 178 366 468  59 469  28 447  43 411 404 511\n",
            " 100 224 500 315 172 217 421 441 253 303 284 445 460 270  74 417 262  75\n",
            " 233  12 493  96 375  97 503 462 399 216 144 438 189 454 494  61 378 190\n",
            " 243 430 320 266 382 139 136 292 322  26 261 439 398 109 278 152 406 479\n",
            " 409 126   5 312 120 165  15 475]\n",
            "Processing layer 10, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.2165, 0.1655, 0.1092, 0.1118, 0.2380, 0.2389, 0.1333, 0.0821, 0.2683,\n",
            "        0.2058, 0.2377, 0.2133, 0.1248, 0.2224, 0.1343, 0.1005, 0.2657, 0.4099,\n",
            "        0.2388, 0.1221, 0.3037, 0.1611, 0.0904, 0.2301, 0.3601, 0.1127, 0.1877,\n",
            "        0.1501, 0.2677, 0.1764, 0.3266, 0.2333, 0.1820, 0.2803, 0.1558, 0.3600,\n",
            "        0.2290, 0.1672, 0.1394, 0.2763, 0.2510, 0.1701, 0.2677, 0.0868, 0.2121,\n",
            "        0.2786, 0.1288, 0.1214, 0.1610, 0.1002, 0.1176, 0.2254, 0.1905, 0.1733,\n",
            "        0.1800, 0.1010, 0.2135, 0.0969, 0.3816, 0.2039, 0.3446, 0.1760, 0.1438,\n",
            "        0.2137, 0.2418, 0.2570, 0.2070, 0.2946, 0.1878, 0.1347, 0.1832, 0.3564,\n",
            "        0.3204, 0.1742, 0.1397, 0.0489, 0.2968, 0.1125, 0.2252, 0.2224, 0.1814,\n",
            "        0.2664, 0.2635, 0.1027, 0.1005, 0.1399, 0.1923, 0.2079, 0.3338, 0.2508,\n",
            "        0.2665, 0.3792, 0.1791, 0.1538, 0.3715, 0.1575, 0.2119, 0.2816, 0.3391,\n",
            "        0.2550, 0.2372, 0.1549, 0.1122, 0.1947, 0.1386, 0.0769, 0.1901, 0.2958,\n",
            "        0.1508, 0.2337, 0.1588, 0.4090, 0.3452, 0.1623, 0.4358, 0.1049, 0.3342,\n",
            "        0.2395, 0.2031, 0.2206, 0.2802, 0.1562, 0.1425, 0.1442, 0.2523, 0.2203,\n",
            "        0.1787, 0.4532, 0.0960, 0.2759, 0.1547, 0.2078, 0.0993, 0.2362, 0.1569,\n",
            "        0.1109, 0.1845, 0.2012, 0.2896, 0.2719, 0.2123, 0.2282, 0.3685, 0.1502,\n",
            "        0.0659, 0.2880, 0.2420, 0.2299, 0.1102, 0.2631, 0.1682, 0.1409, 0.1908,\n",
            "        0.2876, 0.1063, 0.2210, 0.1816, 0.2166, 0.0962, 0.0794, 0.2899, 0.2388,\n",
            "        0.0555, 0.1606, 0.1050, 0.3117, 0.1453, 0.1414, 0.3491, 0.3401, 0.1995,\n",
            "        0.1895, 0.3286, 0.1538, 0.1954, 0.4089, 0.1919, 0.1675, 0.2139, 0.2499,\n",
            "        0.2051, 0.3919, 0.1265, 0.2362, 0.1870, 0.2205, 0.2274, 0.2172, 0.1140,\n",
            "        0.0944, 0.1862, 0.2680, 0.2937, 0.2302, 0.1482, 0.1862, 0.0976, 0.1570,\n",
            "        0.1990, 0.3405, 0.0883, 0.2558, 0.1107, 0.1063, 0.1117, 0.0954, 0.1026,\n",
            "        0.1782, 0.0648, 0.2746, 0.4012, 0.0670, 0.2031, 0.1756, 0.2209, 0.1273,\n",
            "        0.1179, 0.3517, 0.1222, 0.0823, 0.2724, 0.2197, 0.2910, 0.0852, 0.1573,\n",
            "        0.1489, 0.0824, 0.1919, 0.1820, 0.2420, 0.1912, 0.2863, 0.2785, 0.1404,\n",
            "        0.2381, 0.2252, 0.2125, 0.2367, 0.2020, 0.2916, 0.1895, 0.1673, 0.3414,\n",
            "        0.1924, 0.1661, 0.2618, 0.1180, 0.0995, 0.1404, 0.1727, 0.1393, 0.1220,\n",
            "        0.2324, 0.1235, 0.1920, 0.2933, 0.1586, 0.2963, 0.2203, 0.1226, 0.1027,\n",
            "        0.2459, 0.1462, 0.2878, 0.2133, 0.1224, 0.1854, 0.3591, 0.1633, 0.1967,\n",
            "        0.2107, 0.1459, 0.4421, 0.3009, 0.2093, 0.1812, 0.2000, 0.0985, 0.2205,\n",
            "        0.1621, 0.0799, 0.2245, 0.1512, 0.2640, 0.2263, 0.1944, 0.2204, 0.1178,\n",
            "        0.2142, 0.2485, 0.0793, 0.3686, 0.1405, 0.2744, 0.1416, 0.2696, 0.1226,\n",
            "        0.3804, 0.0989, 0.1955, 0.1083, 0.3375, 0.2920, 0.1262, 0.2096, 0.2392,\n",
            "        0.2126, 0.1531, 0.1636, 0.2122, 0.0848, 0.2236, 0.1530, 0.1830, 0.2063,\n",
            "        0.2679, 0.1280, 0.2311, 0.1262, 0.2328, 0.2119, 0.1670, 0.2110, 0.1326,\n",
            "        0.1953, 0.1207, 0.0880, 0.2335, 0.3178, 0.2927, 0.1611, 0.1807, 0.1313,\n",
            "        0.4765, 0.1823, 0.2347, 0.2227, 0.1427, 0.2736, 0.1776, 0.1888, 0.4214,\n",
            "        0.1304, 0.2957, 0.1859, 0.1460, 0.1383, 0.3673, 0.1429, 0.1578, 0.1996,\n",
            "        0.1358, 0.1826, 0.1484, 0.2606, 0.1510, 0.1857, 0.2155, 0.2175, 0.2214,\n",
            "        0.1008, 0.1107, 0.0933, 0.3204, 0.1807, 0.2247, 0.2056, 0.1611, 0.1865,\n",
            "        0.1965, 0.1686, 0.2092, 0.0789, 0.2313, 0.1867, 0.1220, 0.2238, 0.1288,\n",
            "        0.1928, 0.2094, 0.2532, 0.2768, 0.1445, 0.2680, 0.2642, 0.2168, 0.3820,\n",
            "        0.0649, 0.3526, 0.1948, 0.1799, 0.2422, 0.2405, 0.1740, 0.1014, 0.1286,\n",
            "        0.1768, 0.2527, 0.2948, 0.3333, 0.3238, 0.1592, 0.1432, 0.2678, 0.1178,\n",
            "        0.2457, 0.3535, 0.0707, 0.2074, 0.2488, 0.2214, 0.2608, 0.1548, 0.1666,\n",
            "        0.1045, 0.3114, 0.2381, 0.3038, 0.1556, 0.1822, 0.1309, 0.4459, 0.2837,\n",
            "        0.1509, 0.1105, 0.0923, 0.2723, 0.1655, 0.1620, 0.2042, 0.1761, 0.1369,\n",
            "        0.2603, 0.1753, 0.1184, 0.2217, 0.1888, 0.1617, 0.2324, 0.2689, 0.2479,\n",
            "        0.2018, 0.0935, 0.2197, 0.1987, 0.1445, 0.1263, 0.2339, 0.1896, 0.1784,\n",
            "        0.2743, 0.2026, 0.2446, 0.1539, 0.1043, 0.2664, 0.0895, 0.2743, 0.2060,\n",
            "        0.1165, 0.1717, 0.3347, 0.2431, 0.2176, 0.1585, 0.1810, 0.2375, 0.1424,\n",
            "        0.2585, 0.3107, 0.2798, 0.2463, 0.1716, 0.1164, 0.1523, 0.1790, 0.1492,\n",
            "        0.2195, 0.0838, 0.1493, 0.0556, 0.1804, 0.1351, 0.3165, 0.0860, 0.3494,\n",
            "        0.1016, 0.0661, 0.2375, 0.2051, 0.2948, 0.2549, 0.1356, 0.0216, 0.1541,\n",
            "        0.0759, 0.2579, 0.1702, 0.2143, 0.2738, 0.2273, 0.2417, 0.1046, 0.2285,\n",
            "        0.1642, 0.2096, 0.1603, 0.2183, 0.1828, 0.1878, 0.2456, 0.2340],\n",
            "       device='cuda:0')\n",
            "rank [333 127 421 272 114 341  17 111 175 210 181 386  58 297  91  94 291 142\n",
            " 347  24  35 267  71 406 388 217 485 168 112  60 242 199 169  98 301 461\n",
            " 116  88 399 172  30 400  72 363 328 483 165 415 469 417  20 273  76 257\n",
            " 107 343 398 490  67 192 255 329 302 239 222 160 138 145 263 153 231 422\n",
            "  97  33 120 470  45 232 381  39 129 209 293 450 457 499 338 220 426 139\n",
            " 295 439   8 383 191 315 403  42  28  90 455  81  16 384 283  82 149 245\n",
            " 411 354 432 468 496  65 201  99 491 380 397 124  40  89 179 409 289 440\n",
            " 471 261 405 510 452 462 391 229 146  64 501 392 117 305   5 161  18 234\n",
            " 416   4  10 466 488 100 237 183 133 335 511 447 109 327  31 319 438 252\n",
            " 373 317 193  23 147  36 503 141 186 500 284  51 235  78 365 281 376 311\n",
            " 336  13  79 435 410 359 155 214 119 185 278 286 258 125 443 221 477 507\n",
            " 463 358 187 385 157   0 357 498 288 178  63  56 264  11 306 236 140 309\n",
            "  44 320  96 322 270 304 505 379 274 371  87 131 408  66 314 458   9 366\n",
            " 180 489 429  59 212 118 451 238 441 137 276 350 170 198 444 269 369 299\n",
            " 174 324 389 103 285 378 243  86 254 227 176 230 152  52 106 448 240 171\n",
            " 340 436 509  68  26 184 374 368 190 195 344 356 266 136  70 313 508 352\n",
            " 334 419  32 228 156  80 275 465 364 331 481  54 390  92 475 126 449 207\n",
            " 339 396  29 430  61 213 433  73 393  53 249 460 472 497  41 370 150 177\n",
            " 241  37 321 413 244 427   1 504 308 268 113 279 428 437  21 367 330  48\n",
            " 163 506 401 110 256 464 349  95 224 197 134 121  34 418 101 412 130 494\n",
            " 453  93 173 307 312 474 282 355 423 108 143  27 479 476 225 353 194 262\n",
            " 345 271 166 445 382 123  62 402 348 337 122 467 294 167 151 292 233 248\n",
            "  85  74  38 250 104 346 431 351 492 482  69  14   6 323 332 420 342 377\n",
            "  46 395 316 215 182 446 318 303  12 253 259 296 265 218  19 375 251  47\n",
            " 325 434 246 216 287 404  50 459 473 188  25  77 102   3 204 135 361 202\n",
            " 424 148   2 300 154 203 164 115 502 414 454  83 260 206 486 394  55 360\n",
            "  15  84  49 247 132 298 277 196  57 158 128 205 189 442 362 425  22 456\n",
            " 200 326  43 484 223 310 478 226 219   7 280 159 290 372 105 495 407 211\n",
            " 487 144 387 208 480 162  75 493]\n",
            "Processing layer 11, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.0951, 0.2325, 0.0961, 0.0995, 0.1230, 0.1520, 0.1451, 0.1648, 0.1684,\n",
            "        0.1152, 0.1516, 0.1674, 0.1253, 0.0473, 0.1736, 0.1024, 0.1095, 0.1521,\n",
            "        0.1869, 0.0708, 0.1774, 0.1469, 0.1574, 0.1329, 0.1172, 0.1392, 0.1310,\n",
            "        0.1913, 0.1274, 0.1891, 0.1134, 0.1324, 0.1127, 0.1602, 0.1473, 0.1023,\n",
            "        0.1027, 0.1051, 0.1289, 0.1193, 0.1575, 0.1064, 0.1220, 0.1498, 0.1549,\n",
            "        0.1070, 0.2074, 0.1408, 0.1239, 0.0965, 0.1297, 0.1076, 0.1724, 0.1028,\n",
            "        0.1771, 0.2038, 0.1374, 0.0953, 0.1312, 0.0592, 0.1064, 0.1122, 0.1218,\n",
            "        0.1319, 0.1561, 0.1076, 0.0869, 0.1221, 0.0705, 0.0787, 0.1687, 0.1294,\n",
            "        0.0714, 0.0919, 0.1055, 0.1457, 0.1504, 0.1462, 0.1801, 0.1244, 0.1582,\n",
            "        0.0863, 0.1014, 0.1823, 0.1173, 0.2035, 0.0918, 0.1653, 0.1128, 0.1883,\n",
            "        0.1847, 0.2312, 0.1252, 0.1437, 0.0830, 0.1233, 0.0948, 0.0975, 0.1411,\n",
            "        0.0840, 0.0993, 0.1100, 0.0968, 0.2261, 0.1211, 0.1070, 0.1354, 0.0724,\n",
            "        0.1170, 0.0879, 0.0853, 0.0889, 0.2028, 0.1429, 0.1201, 0.1642, 0.1424,\n",
            "        0.1139, 0.1780, 0.0920, 0.1984, 0.1178, 0.1439, 0.1523, 0.0926, 0.1143,\n",
            "        0.1415, 0.1405, 0.1045, 0.0859, 0.1024, 0.2357, 0.1473, 0.1557, 0.1196,\n",
            "        0.0773, 0.1121, 0.1401, 0.1543, 0.1029, 0.1961, 0.0870, 0.1515, 0.1034,\n",
            "        0.0831, 0.1188, 0.1290, 0.1538, 0.0918, 0.1555, 0.1137, 0.0983, 0.0893,\n",
            "        0.1163, 0.1823, 0.1173, 0.1273, 0.1027, 0.1022, 0.2124, 0.0913, 0.0674,\n",
            "        0.1704, 0.1065, 0.1611, 0.0807, 0.1499, 0.1146, 0.1474, 0.1202, 0.1571,\n",
            "        0.2203, 0.1003, 0.1300, 0.1562, 0.1840, 0.0865, 0.1317, 0.1776, 0.0883,\n",
            "        0.1658, 0.1164, 0.1709, 0.1272, 0.0661, 0.0961, 0.0820, 0.2089, 0.1306,\n",
            "        0.1703, 0.1461, 0.1061, 0.1674, 0.1289, 0.1589, 0.1321, 0.1213, 0.1441,\n",
            "        0.1141, 0.1978, 0.2767, 0.0922, 0.1302, 0.1029, 0.1886, 0.2269, 0.1707,\n",
            "        0.1339, 0.0668, 0.1365, 0.2038, 0.1161, 0.1300, 0.1238, 0.1757, 0.1209,\n",
            "        0.0909, 0.0941, 0.1636, 0.1378, 0.0975, 0.0845, 0.1810, 0.0828, 0.1290,\n",
            "        0.2150, 0.1252, 0.0923, 0.0951, 0.1488, 0.1001, 0.1052, 0.1324, 0.1633,\n",
            "        0.1545, 0.0840, 0.1121, 0.0992, 0.2234, 0.0870, 0.0934, 0.0955, 0.1533,\n",
            "        0.1644, 0.1093, 0.1007, 0.1274, 0.1250, 0.1167, 0.1116, 0.1889, 0.1886,\n",
            "        0.1348, 0.1723, 0.0900, 0.1508, 0.1188, 0.2085, 0.1025, 0.1903, 0.1646,\n",
            "        0.1177, 0.1737, 0.1821, 0.1429, 0.1001, 0.0903, 0.1458, 0.0873, 0.0885,\n",
            "        0.1017, 0.1336, 0.0839, 0.1479, 0.1132, 0.1913, 0.1258, 0.1658, 0.1130,\n",
            "        0.0945, 0.0851, 0.1787, 0.1034, 0.1393, 0.1397, 0.0933, 0.1132, 0.0985,\n",
            "        0.0881, 0.1495, 0.1078, 0.1290, 0.1664, 0.1324, 0.1427, 0.1028, 0.2257,\n",
            "        0.1353, 0.0974, 0.1210, 0.0804, 0.1573, 0.1608, 0.1272, 0.1371, 0.1013,\n",
            "        0.1377, 0.2369, 0.1529, 0.1491, 0.1541, 0.1452, 0.1230, 0.1593, 0.1128,\n",
            "        0.1778, 0.1248, 0.1413, 0.1480, 0.1546, 0.1860, 0.1067, 0.1049, 0.1042,\n",
            "        0.1463, 0.1103, 0.1185, 0.1113, 0.1472, 0.1428, 0.1108, 0.1132, 0.1047,\n",
            "        0.1559, 0.0958, 0.1261, 0.1310, 0.1553, 0.1204, 0.1082, 0.1669, 0.1710,\n",
            "        0.1428, 0.1805, 0.2099, 0.1159, 0.1199, 0.1357, 0.1058, 0.1375, 0.1564,\n",
            "        0.1199, 0.1571, 0.2017, 0.0725, 0.0884, 0.1029, 0.1267, 0.2013, 0.1101,\n",
            "        0.2440, 0.1710, 0.1473, 0.1259, 0.2102, 0.1374, 0.1649, 0.0964, 0.1164,\n",
            "        0.1128, 0.0949, 0.1347, 0.0822, 0.0993, 0.1561, 0.1206, 0.2059, 0.1445,\n",
            "        0.1349, 0.1216, 0.1465, 0.1107, 0.1417, 0.2153, 0.1484, 0.1320, 0.1708,\n",
            "        0.1127, 0.1149, 0.0832, 0.1752, 0.1099, 0.1008, 0.1294, 0.1111, 0.1578,\n",
            "        0.1050, 0.2039, 0.1187, 0.1115, 0.1476, 0.1696, 0.1331, 0.1138, 0.0765,\n",
            "        0.2005, 0.1222, 0.1696, 0.1177, 0.1003, 0.1690, 0.0845, 0.1558, 0.1258,\n",
            "        0.1260, 0.0865, 0.1162, 0.1532, 0.1212, 0.1723, 0.1053, 0.0916, 0.2380,\n",
            "        0.1319, 0.1416, 0.1302, 0.1313, 0.1740, 0.1196, 0.0925, 0.1695, 0.1427,\n",
            "        0.1307, 0.1386, 0.0901, 0.0547, 0.1298, 0.1765, 0.0760, 0.1272, 0.1624,\n",
            "        0.0739, 0.1076, 0.0994, 0.0988, 0.1140, 0.0970, 0.0981, 0.1077, 0.1260,\n",
            "        0.1242, 0.0926, 0.1125, 0.1506, 0.1453, 0.1213, 0.1058, 0.1819, 0.0737,\n",
            "        0.1075, 0.1940, 0.0761, 0.0874, 0.1616, 0.1002, 0.0943, 0.1220, 0.1384,\n",
            "        0.1318, 0.1199, 0.1748, 0.1014, 0.0921, 0.2048, 0.1606, 0.1907, 0.1241,\n",
            "        0.1434, 0.1355, 0.1089, 0.1070, 0.1469, 0.1107, 0.1223, 0.0846, 0.1301,\n",
            "        0.1023, 0.1084, 0.1902, 0.1590, 0.1513, 0.1813, 0.0863, 0.1296, 0.1501,\n",
            "        0.1167, 0.1120, 0.1510, 0.1531, 0.0760, 0.1323, 0.1284, 0.0942, 0.2079,\n",
            "        0.1167, 0.1280, 0.0942, 0.1309, 0.1370, 0.1489, 0.1143, 0.0497],\n",
            "       device='cuda:0')\n",
            "rank [200 360 422 307 131   1  91 205 103 296 238 171 383 225 159 364 344 187\n",
            " 257 503  46 376 473 397 210  55  85 112 353 358 405 120 199 140 460  27\n",
            " 275 475 259 488  29 250 251 204  89  18 320  90 175  83 154 263 457 491\n",
            " 222 343  78 281 118 315 178  20  54 437 214 390 470 427 262  14  52 253\n",
            " 419 361 341 182 386 206 162 189 407 401 430 410  70   8  11 192 340 292\n",
            " 180 277  87 366   7 260 243 115 218 233 440 463 164 302 474  33 313 489\n",
            " 194  80 395  40  22 301 352 170 350 174 374  64 333 412 133 149 337  44\n",
            " 319 234 138 310 147 242 417 498 308 123  17   5  10 142 490 497 255 453\n",
            "  76 494 166  43 289 309 509 229 384 318 273 400 168  34 132 362 328  21\n",
            " 481 380 324  77 190 267  75 454 311   6 377 197 122  93 477 113 264 329\n",
            " 342 294 431 116 382 424 126 317  98  47 127 137 284 283  25 433 467 219\n",
            " 306 349  56 365 304 508 209 347 478 106 297 378 252 371 207 271 402  23\n",
            "  31 232 293 500 195 385  63 423 468 177 426  58 336  26 507 432 188 425\n",
            " 202 485 173 212 436  50 493 393  71 146 291 224 193  38 501 505  28 246\n",
            " 156 183 439 303 357 335 449 414 363 276 413  12  92 226 247 316  79 450\n",
            " 476  48 213  95 312   4 483 406  67  42 466  62 379 455 196 418 104 299\n",
            " 215 375 338 169 114 346 469 351 428 134  39 145 256 398 326 121 261 408\n",
            " 155  84  24 108 248 495 504 368 181 153 416 211 345   9 388 167 510 125\n",
            " 198 445 117 403 150  30 274 331 286 278 314 369  88  32 387 452  61 236\n",
            " 136 496 249 399 327 394 330 482 381 325 359 101 391  16 244 479 487 339\n",
            " 290 448 442  65  51 459  45 105 480 321 163  60  41 191 348 456  74 420\n",
            " 231  37 396 322 332 128 323 143 282 356 203 139  53 295 157  36 258  15\n",
            " 130 486  35 158 270 471  82 305 392 245 409 172 464 265 230   3 443 373\n",
            " 100 237 444 287 151 447  97 220 298 446 102  49 367 185   2 334 241  57\n",
            "   0 228 370  96 279 465 502 506 217 240 285 451 124 429 227 201 472 119\n",
            "  73  86 148 421 160 216 266 434 254 152 111 269 355 179 288 109 462 268\n",
            " 141 239  66 415 176 492  81 129 110 280 484 221 411  99 235 272 389 144\n",
            "  94 223 372 186 165 300  69 135 404 461 438 499 441 458 354 107  72  19\n",
            "  68 161 208 184  59 435 511  13]\n",
            "Processing layer 12, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.1113, 0.1202, 0.0747, 0.1639, 0.1122, 0.0804, 0.1237, 0.1653, 0.0645,\n",
            "        0.0994, 0.1004, 0.0996, 0.1222, 0.1242, 0.0977, 0.0709, 0.0809, 0.1125,\n",
            "        0.0975, 0.0950, 0.1096, 0.1232, 0.1498, 0.0997, 0.1006, 0.0687, 0.1178,\n",
            "        0.0629, 0.0908, 0.0908, 0.1247, 0.1389, 0.0761, 0.0641, 0.0851, 0.0852,\n",
            "        0.1118, 0.1336, 0.0966, 0.1309, 0.0827, 0.1320, 0.0851, 0.1443, 0.1230,\n",
            "        0.0530, 0.1121, 0.1084, 0.0790, 0.1525, 0.1056, 0.0967, 0.1534, 0.1196,\n",
            "        0.0970, 0.0776, 0.1231, 0.1229, 0.1087, 0.1039, 0.0863, 0.0591, 0.1065,\n",
            "        0.0736, 0.0827, 0.0755, 0.0916, 0.1177, 0.0696, 0.1330, 0.1254, 0.0719,\n",
            "        0.1054, 0.1565, 0.1438, 0.0975, 0.1313, 0.1029, 0.0896, 0.0584, 0.0903,\n",
            "        0.1058, 0.1726, 0.1201, 0.0839, 0.0811, 0.1441, 0.0796, 0.1125, 0.0874,\n",
            "        0.1233, 0.1144, 0.0795, 0.1156, 0.1030, 0.1447, 0.1427, 0.0950, 0.1183,\n",
            "        0.0937, 0.1397, 0.1065, 0.1333, 0.1178, 0.0701, 0.0625, 0.1104, 0.1737,\n",
            "        0.1006, 0.0792, 0.1265, 0.1247, 0.1295, 0.1058, 0.2030, 0.0922, 0.0934,\n",
            "        0.0787, 0.0769, 0.0843, 0.1072, 0.0843, 0.1105, 0.1260, 0.1146, 0.1268,\n",
            "        0.1261, 0.1775, 0.1683, 0.1141, 0.1060, 0.1557, 0.0805, 0.1071, 0.0859,\n",
            "        0.1477, 0.1625, 0.1394, 0.0741, 0.1894, 0.0861, 0.1278, 0.1618, 0.0992,\n",
            "        0.0935, 0.0868, 0.1311, 0.1284, 0.1409, 0.1585, 0.1076, 0.1794, 0.0976,\n",
            "        0.0907, 0.0923, 0.1067, 0.0951, 0.1463, 0.1088, 0.1338, 0.1759, 0.1438,\n",
            "        0.1155, 0.0952, 0.0801, 0.1271, 0.1626, 0.1369, 0.1206, 0.1174, 0.1068,\n",
            "        0.0854, 0.1196, 0.0851, 0.1091, 0.1645, 0.1382, 0.1301, 0.0834, 0.1086,\n",
            "        0.1675, 0.1267, 0.1304, 0.0577, 0.1243, 0.0594, 0.1063, 0.1424, 0.1160,\n",
            "        0.0538, 0.2079, 0.1078, 0.0653, 0.1028, 0.0827, 0.1412, 0.0832, 0.1228,\n",
            "        0.1193, 0.1449, 0.0995, 0.0702, 0.1388, 0.1183, 0.1035, 0.1296, 0.1352,\n",
            "        0.1050, 0.0813, 0.0534, 0.1231, 0.0971, 0.1140, 0.1318, 0.1307, 0.1121,\n",
            "        0.0872, 0.0682, 0.1380, 0.1064, 0.0891, 0.0696, 0.1233, 0.0978, 0.1055,\n",
            "        0.1234, 0.0726, 0.0841, 0.0736, 0.0971, 0.1254, 0.0887, 0.1104, 0.0921,\n",
            "        0.0906, 0.1082, 0.1414, 0.0937, 0.0963, 0.1140, 0.1155, 0.1298, 0.1196,\n",
            "        0.0814, 0.0901, 0.1201, 0.1102, 0.0719, 0.1495, 0.1009, 0.0957, 0.1127,\n",
            "        0.1735, 0.1273, 0.1115, 0.1055, 0.0877, 0.0935, 0.1209, 0.1350, 0.1460,\n",
            "        0.1248, 0.1100, 0.1051, 0.1167, 0.0986, 0.1201, 0.0778, 0.1435, 0.1282,\n",
            "        0.0869, 0.1124, 0.1121, 0.0968, 0.1323, 0.1137, 0.0827, 0.0722, 0.0775,\n",
            "        0.1401, 0.1146, 0.0983, 0.1221, 0.0791, 0.0937, 0.1874, 0.0890, 0.0715,\n",
            "        0.1156, 0.1351, 0.1535, 0.0841, 0.0961, 0.0816, 0.1037, 0.0725, 0.1775,\n",
            "        0.1361, 0.1041, 0.1047, 0.0868, 0.0873, 0.0912, 0.1235, 0.1087, 0.0911,\n",
            "        0.0983, 0.1303, 0.1042, 0.0373, 0.0675, 0.1008, 0.1094, 0.0966, 0.1410,\n",
            "        0.1079, 0.0989, 0.0671, 0.1331, 0.1900, 0.0946, 0.0813, 0.1000, 0.1249,\n",
            "        0.1704, 0.1094, 0.0974, 0.1047, 0.1215, 0.0881, 0.0938, 0.0739, 0.0676,\n",
            "        0.1411, 0.1260, 0.1788, 0.1111, 0.1266, 0.1446, 0.0971, 0.1037, 0.0963,\n",
            "        0.1171, 0.1665, 0.1079, 0.1519, 0.0980, 0.1330, 0.1080, 0.1282, 0.1657,\n",
            "        0.0865, 0.1163, 0.1316, 0.1523, 0.1479, 0.1315, 0.1460, 0.0905, 0.1078,\n",
            "        0.1181, 0.1307, 0.0931, 0.0996, 0.1487, 0.0904, 0.0917, 0.0833, 0.1052,\n",
            "        0.0995, 0.1328, 0.0957, 0.0833, 0.0917, 0.1205, 0.1010, 0.1224, 0.1235,\n",
            "        0.1472, 0.0817, 0.1219, 0.0593, 0.1196, 0.1691, 0.0889, 0.1227, 0.1086,\n",
            "        0.1498, 0.0582, 0.1393, 0.1246, 0.0929, 0.1445, 0.1250, 0.0731, 0.1183,\n",
            "        0.0967, 0.0747, 0.1193, 0.0910, 0.1159, 0.1609, 0.1557, 0.0954, 0.1023,\n",
            "        0.1349, 0.0953, 0.1884, 0.0933, 0.1130, 0.1260, 0.0892, 0.1173, 0.1090,\n",
            "        0.1276, 0.1316, 0.1359, 0.1503, 0.1553, 0.0754, 0.1044, 0.0932, 0.1369,\n",
            "        0.1344, 0.1047, 0.1061, 0.1004, 0.0634, 0.1046, 0.0998, 0.1031, 0.1262,\n",
            "        0.1379, 0.0671, 0.0938, 0.2008, 0.1051, 0.1563, 0.1206, 0.0718, 0.1641,\n",
            "        0.0635, 0.1304, 0.1050, 0.1332, 0.1381, 0.0842, 0.0732, 0.1201, 0.0705,\n",
            "        0.0878, 0.1108, 0.1080, 0.0711, 0.1449, 0.1257, 0.0879, 0.2023, 0.1259,\n",
            "        0.1024, 0.1105, 0.0931, 0.0971, 0.1464, 0.1202, 0.0783, 0.0808, 0.1207,\n",
            "        0.1598, 0.0817, 0.0914, 0.0976, 0.0976, 0.0994, 0.0880, 0.0799, 0.1082,\n",
            "        0.1244, 0.1220, 0.1550, 0.1809, 0.1228, 0.1703, 0.0894, 0.1076, 0.1176,\n",
            "        0.1092, 0.0986, 0.1068, 0.1297, 0.0810, 0.0815, 0.1048, 0.1038, 0.1772,\n",
            "        0.1913, 0.0857, 0.1128, 0.1373, 0.1145, 0.1516, 0.1179, 0.0616, 0.0743,\n",
            "        0.0959, 0.1179, 0.1183, 0.0974, 0.1131, 0.1300, 0.1442, 0.0901],\n",
            "       device='cuda:0')\n",
            "rank [190 114 457 435 495 319 139 407 285 480 151 335 127 296 494 160 107 252\n",
            "  82 324 482 383 128 180 343 350   7 175 440   3 166 136 142 401 468 149\n",
            "  73 437 131 402 418 479 290  52  49 354 345 500 417 387  22 248 364 355\n",
            " 135 378 463 157 260 357 454 199  95 338 392  43 510  86 161  74 268  96\n",
            " 187 236 195 333 314 148 279 100 137 389  31 202 176 445 218 432 498 167\n",
            " 422 297 416 206 289 259 405 423 159  37 102 444 318 347  69 370 274  41\n",
            " 213 415 353 356  76 146  39 214 361 442 182 307 177 509 241 489 205 112\n",
            " 147 349 269 141 414 253 165 125 181 337 110 431 126 123 410 334 458 455\n",
            " 230  70 393 323 261  30 111 390 477 184  13   6 303 377 225 222  90  21\n",
            "  56 210  44  57 197 481 385 376  12 282 478 380 328 258 467 168 438 374\n",
            " 464   1 266 448 245  83 382 242 172  53 398 198 203 395 506  98 360 505\n",
            " 501 103  26  67 485 169 412 342 264 352 188 400  93 288 240 162 280 124\n",
            " 499  91 129 239 212 275 508 409 497 251  88  17 271   4 272 215  46  36\n",
            " 254   0 336 451 460 122 232 106 246 262  20 312 325 486 174 413 158 304\n",
            "  58 386 179  47 235 476 452 348 315 344 191 359 150 484 120 133 488 170\n",
            " 155  62 101 219 186 425 130 113  81  50 224 255  72 368 436 263 207 443\n",
            " 492 424 327 299 428 420 308 298  59 493 340 294 204 430  94  77 193 459\n",
            " 404 375 249 311 108  24  10 426 322 429  23 363  11 200 369 473   9 143\n",
            " 316 265 487 306 281 346 223  14 472 471 152  18  75 507 326 211 339 462\n",
            " 229  54 273  51 396  38 313 341 238 292 504 250 371 403 406 163 156  19\n",
            "  97 320 434 330 237 284  99 144 257 116 408 421 362 461 391 154 115 233\n",
            " 366 373  66 470 302 305 399  29  28 153 234 358 365  80 244 511  78 483\n",
            " 411 220 286 384 231 329 474 456 450 256  89 301 216 270 300 145 351  60\n",
            " 140 134 496 171  35 173  42  34 121 119 446 227 291  84 178 367 372 196\n",
            " 194  64  40 276 469 379 293 491 243 321 208  85 490  16 466 132   5 164\n",
            " 475  87  92 109 283  48 117 465 267  55 278 118  32  65 419   2 397 503\n",
            " 138 331  63 228 447 394 226 295 277 247  71 439 287 453  15 449 201 104\n",
            "  68 221  25 217 332 310 317 433 192   8  33 441 427  27 105 502 185 381\n",
            "  61  79 388 183 189 209  45 309]\n",
            "Processing layer 13, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.0939, 0.1464, 0.1735, 0.1191, 0.0876, 0.0956, 0.0803, 0.1136, 0.1021,\n",
            "        0.0886, 0.0798, 0.0565, 0.0771, 0.1121, 0.0886, 0.1321, 0.0897, 0.0893,\n",
            "        0.1254, 0.1145, 0.0757, 0.0773, 0.1128, 0.0897, 0.0765, 0.0771, 0.0982,\n",
            "        0.1377, 0.1079, 0.1418, 0.1043, 0.1378, 0.1426, 0.0854, 0.1273, 0.0983,\n",
            "        0.1007, 0.1274, 0.1020, 0.0985, 0.1326, 0.1535, 0.0945, 0.1628, 0.0721,\n",
            "        0.0786, 0.1340, 0.1156, 0.1076, 0.0924, 0.1394, 0.0818, 0.0917, 0.0913,\n",
            "        0.1089, 0.1080, 0.1689, 0.1264, 0.0490, 0.0978, 0.1032, 0.1009, 0.1025,\n",
            "        0.1051, 0.1256, 0.1243, 0.1090, 0.0894, 0.0749, 0.1069, 0.1435, 0.1930,\n",
            "        0.0932, 0.2054, 0.1406, 0.1741, 0.1179, 0.0871, 0.0832, 0.0928, 0.0782,\n",
            "        0.1002, 0.0894, 0.1294, 0.0883, 0.1098, 0.0768, 0.0929, 0.0805, 0.1208,\n",
            "        0.1323, 0.1488, 0.1189, 0.1130, 0.1290, 0.1947, 0.0914, 0.1716, 0.2356,\n",
            "        0.1498, 0.1058, 0.1045, 0.1048, 0.1676, 0.1368, 0.1543, 0.0663, 0.0915,\n",
            "        0.0958, 0.0793, 0.0905, 0.0785, 0.0845, 0.1229, 0.1458, 0.0532, 0.1230,\n",
            "        0.0701, 0.1953, 0.0591, 0.1041, 0.1104, 0.0981, 0.1609, 0.1330, 0.1410,\n",
            "        0.1051, 0.1666, 0.0934, 0.1153, 0.2215, 0.0878, 0.1382, 0.0952, 0.1197,\n",
            "        0.0777, 0.1122, 0.2139, 0.1682, 0.0695, 0.0963, 0.0913, 0.0881, 0.0651,\n",
            "        0.0647, 0.0697, 0.1107, 0.0791, 0.0963, 0.1047, 0.0826, 0.1135, 0.0684,\n",
            "        0.2029, 0.1395, 0.1167, 0.1447, 0.1040, 0.0695, 0.1246, 0.1215, 0.0794,\n",
            "        0.1374, 0.0962, 0.0936, 0.1067, 0.1226, 0.1350, 0.1340, 0.1268, 0.0712,\n",
            "        0.1291, 0.2262, 0.1214, 0.1354, 0.1373, 0.0767, 0.0925, 0.0982, 0.0736,\n",
            "        0.0883, 0.1043, 0.1634, 0.2313, 0.0681, 0.1005, 0.1021, 0.1016, 0.1505,\n",
            "        0.1073, 0.0918, 0.1070, 0.0819, 0.0702, 0.1145, 0.0864, 0.0788, 0.0725,\n",
            "        0.1494, 0.1390, 0.0933, 0.0522, 0.0954, 0.1457, 0.1316, 0.0769, 0.1500,\n",
            "        0.1428, 0.0722, 0.0698, 0.0862, 0.1052, 0.1430, 0.1328, 0.1988, 0.0815,\n",
            "        0.0682, 0.1406, 0.0625, 0.1082, 0.1250, 0.0752, 0.1434, 0.1479, 0.1070,\n",
            "        0.1550, 0.1625, 0.1037, 0.1222, 0.1293, 0.1351, 0.1018, 0.1060, 0.1003,\n",
            "        0.1008, 0.1310, 0.1077, 0.1012, 0.1510, 0.0596, 0.0985, 0.0621, 0.0618,\n",
            "        0.0996, 0.0866, 0.1036, 0.0825, 0.1431, 0.0655, 0.1066, 0.0831, 0.0878,\n",
            "        0.1126, 0.1043, 0.1090, 0.1833, 0.0975, 0.0979, 0.0912, 0.0988, 0.1231,\n",
            "        0.1059, 0.1607, 0.0511, 0.0811, 0.1116, 0.1085, 0.0916, 0.1425, 0.1382,\n",
            "        0.1648, 0.1006, 0.1133, 0.1276, 0.1288, 0.0909, 0.0750, 0.0869, 0.0757,\n",
            "        0.0628, 0.0984, 0.0694, 0.1159, 0.1555, 0.0955, 0.1112, 0.1704, 0.1587,\n",
            "        0.1524, 0.1112, 0.0417, 0.0922, 0.1210, 0.0827, 0.1371, 0.0973, 0.1096,\n",
            "        0.1420, 0.1717, 0.0878, 0.1679, 0.0601, 0.1255, 0.1665, 0.1123, 0.1470,\n",
            "        0.0576, 0.1283, 0.1804, 0.1002, 0.1378, 0.1142, 0.1055, 0.0812, 0.0845,\n",
            "        0.0969, 0.0651, 0.0998, 0.1252, 0.1047, 0.0855, 0.1738, 0.1461, 0.1233,\n",
            "        0.0656, 0.1042, 0.1872, 0.1090, 0.1707, 0.1156, 0.0465, 0.1497, 0.0739,\n",
            "        0.0979, 0.1052, 0.1189, 0.1540, 0.1349, 0.1028, 0.1048, 0.1963, 0.1099,\n",
            "        0.1324, 0.0543, 0.1058, 0.1015, 0.1050, 0.1265, 0.0467, 0.0965, 0.0617,\n",
            "        0.0920, 0.0831, 0.1799, 0.1204, 0.1026, 0.0822, 0.2190, 0.1938, 0.1522,\n",
            "        0.1003, 0.1607, 0.1257, 0.0985, 0.0542, 0.0830, 0.1288, 0.2287, 0.1200,\n",
            "        0.1932, 0.1052, 0.0909, 0.1130, 0.0825, 0.0710, 0.1291, 0.0979, 0.1224,\n",
            "        0.1099, 0.0891, 0.1211, 0.0812, 0.0817, 0.0979, 0.0922, 0.1369, 0.1025,\n",
            "        0.0790, 0.1075, 0.0811, 0.1393, 0.1209, 0.0849, 0.1234, 0.1105, 0.1476,\n",
            "        0.0954, 0.1323, 0.1069, 0.0812, 0.0721, 0.0855, 0.0856, 0.1287, 0.1007,\n",
            "        0.0732, 0.1026, 0.1024, 0.2997, 0.0914, 0.0859, 0.1832, 0.1187, 0.1244,\n",
            "        0.1184, 0.1012, 0.0723, 0.1163, 0.0825, 0.1351, 0.1059, 0.1248, 0.1394,\n",
            "        0.1018, 0.1215, 0.1245, 0.2928, 0.0863, 0.0963, 0.1693, 0.0970, 0.1313,\n",
            "        0.1103, 0.0977, 0.0970, 0.1175, 0.0802, 0.1690, 0.1732, 0.1190, 0.1351,\n",
            "        0.1126, 0.0926, 0.1290, 0.0647, 0.1023, 0.1481, 0.1086, 0.0831, 0.0759,\n",
            "        0.2942, 0.0872, 0.1114, 0.0579, 0.1407, 0.1285, 0.1421, 0.1410, 0.1168,\n",
            "        0.0824, 0.1487, 0.1013, 0.0898, 0.1334, 0.0772, 0.1246, 0.0831, 0.1198,\n",
            "        0.1453, 0.1211, 0.1166, 0.1863, 0.0517, 0.1824, 0.1111, 0.1079, 0.1013,\n",
            "        0.1739, 0.0737, 0.1265, 0.1227, 0.1305, 0.0997, 0.0793, 0.2833, 0.1293,\n",
            "        0.1229, 0.0811, 0.1165, 0.1066, 0.0932, 0.0990, 0.1102, 0.0482, 0.0867,\n",
            "        0.1073, 0.0632, 0.1117, 0.0883, 0.0914, 0.1157, 0.1387, 0.0634, 0.1124,\n",
            "        0.0544, 0.1149, 0.1656, 0.0849, 0.1279, 0.0909, 0.1329, 0.0745],\n",
            "       device='cuda:0')\n",
            "rank [408 450 426 484  98 183 367 172 130 357 137  73 153 214 340 118  95 358\n",
            " 369  71 326 471 255 411 473 308 353  75 477 321   2 438 298  97 328 286\n",
            " 429 437  56 138 300 103 127 303 506 270 182  43 226 123 262 361 287 283\n",
            " 225 105 336  41 288 359 238 188 206  99 331 198  91 460 446 223 395 305\n",
            "   1 322 114 203 468 156  70 222 247 212 207  32 268 456 297  29 125 457\n",
            " 454  74 217 154  50 422 390 199 501 269 132  31 310  27 162 175 294 385\n",
            " 104 174 230 440 419 167 337  46 168 463 124 510 213  40 342 397  90  15\n",
            " 204 431 235 481  83 485 229 171 375 443  94 366 274 403 455 307 508 273\n",
            "  37  34 169 479 347  57 362  64 302  18 318 220 421 465 159 425 413  65\n",
            " 393 323 260 116 486 113 480 166 377 228 160 424 173 380 469 292 391  89\n",
            " 354 368 467 134   3 439  92 335 412 414  76 435 458 155 470 488 417 282\n",
            " 500 329  47 129 505  19 194 311   7 151 272 372  93  22 441 252 503 304\n",
            " 136  13 497 265 452 289 285 474 146 394 121 432 492 341 378  85 296 254\n",
            "  66 327  54 447 266 219  55 475  28 236  48 388 189 495 224 191 398  69\n",
            " 165 249 489 232 261 420 100 344 312 370 211 334 126  63 346 102 339 149\n",
            " 319 101 181 253  30 325 120 157 227 245  60 338 355 406  62 386 407 445\n",
            " 186   8  38 423 231 187 345 461 476 237 415  61 234 404  36 271 185 233\n",
            " 360 309  81 317 482 243 491 259 363  39 240 280  35  26 178 122 257 333\n",
            " 376 383  59 433 256 295 430 434 315 349 140 148 428 163 108   5 284 202\n",
            " 396 133  42   0 164 128 200 490  72  87  79 442 177  49 291 384 351 190\n",
            "  52 267 107  96 499 409 141  53 258 371 275 509 110 462  16  23  67  82\n",
            "  17 379  14   9 180 498  84 142 251 131 299   4 451  77 277 494 244 195\n",
            " 427 210 410 402 320 401  33 507 392 314 112  78 448 250 352 466 365 293\n",
            " 150 246 418 373 459 356 192  51 382 215 381 399 313 264 389 487  88   6\n",
            " 436  10 161 483 109 147 387 196  45 111  80 135  21 464  12  25 205  86\n",
            " 176  24 449  20 278 221 276  68 511 332 478 179 405 197 416 208  44 400\n",
            " 170 374 193 117 209 145 158 139 281 152 216 184 106 324 248 143 316 144\n",
            " 444 502 496 279 218 241 242 350 301 239 119 453 306  11 504 343 364 115\n",
            " 201 472 263  58 493 348 330 290]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the model after pruning, without finetuning:\")\n",
        "_, accuracy_model_prune, _ = validate(val_loader, model_prune, criterion)\n",
        "print(f\"This model's accuracy is {accuracy_model_prune}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYYQgJSTod75",
        "outputId": "1304a536-c835-42e5-81e8-00e9e2204387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model after pruning, without finetuning:\n",
            " * Acc@1 79.580 Acc@5 98.380\n",
            "This model's accuracy is 79.57999420166016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(model_prune, train_loader, val_loader, epochs=1, criterion=criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhr2zLsXoKDy",
        "outputId": "04214886-5c0a-4eda-cc7b-68fc242ef89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 10.000 Acc@5 50.000\n",
            "learning_rate: 0.0001\n",
            "Epoch[0](0/391): Loss 2.0290 Prec@1(1,5) 32.03, 78.91 Lr 0.0001\n",
            "Epoch[0](39/391): Loss 1.8653 Prec@1(1,5) 43.65, 87.09 Lr 0.0001\n",
            "Epoch[0](78/391): Loss 1.6173 Prec@1(1,5) 55.03, 91.25 Lr 0.0001\n",
            "Epoch[0](117/391): Loss 1.4574 Prec@1(1,5) 60.12, 93.00 Lr 0.0001\n",
            "Epoch[0](156/391): Loss 1.3378 Prec@1(1,5) 63.39, 94.09 Lr 0.0001\n",
            "Epoch[0](195/391): Loss 1.2434 Prec@1(1,5) 66.09, 94.85 Lr 0.0001\n",
            "Epoch[0](234/391): Loss 1.1696 Prec@1(1,5) 67.98, 95.40 Lr 0.0001\n",
            "Epoch[0](273/391): Loss 1.1052 Prec@1(1,5) 69.68, 95.82 Lr 0.0001\n",
            "Epoch[0](312/391): Loss 1.0546 Prec@1(1,5) 70.97, 96.18 Lr 0.0001\n",
            "Epoch[0](351/391): Loss 1.0121 Prec@1(1,5) 72.01, 96.39 Lr 0.0001\n",
            "Epoch[0](390/391): Loss 0.9718 Prec@1(1,5) 73.06, 96.61 Lr 0.0001\n",
            " * Acc@1 79.580 Acc@5 98.380\n",
            "=>Best accuracy 79.580\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu4): ReLU(inplace=True)\n",
              "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu6): ReLU(inplace=True)\n",
              "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu7): ReLU(inplace=True)\n",
              "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu8): ReLU(inplace=True)\n",
              "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu10): ReLU(inplace=True)\n",
              "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu11): ReLU(inplace=True)\n",
              "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu12): ReLU(inplace=True)\n",
              "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu14): ReLU(inplace=True)\n",
              "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu15): ReLU(inplace=True)\n",
              "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu16): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
              "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Similarity**\n"
      ],
      "metadata": {
        "id": "42pQB-OgvXSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_similarity(model, model_ori):\n",
        "    oristate_dict = model_ori.state_dict()\n",
        "    state_dict = model.state_dict()\n",
        "    last_select_index = None  # Conv index selected in the previous layer\n",
        "\n",
        "    cnt = 0\n",
        "    for name, module in model.named_modules():\n",
        "        name = name.replace('module.', '')\n",
        "\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            cnt += 1\n",
        "            oriweight = oristate_dict[name + '.weight']\n",
        "            curweight = state_dict[name + '.weight']\n",
        "            orifilter_num = oriweight.size(0)\n",
        "            currentfilter_num = curweight.size(0)\n",
        "            print(f\"Processing layer {cnt}, original layer has {orifilter_num} filters, pruning model has {currentfilter_num} filters\")\n",
        "\n",
        "            if orifilter_num != currentfilter_num:\n",
        "                cov_id = cnt\n",
        "                #************ rank the filter's importance here\n",
        "                # print(oristate_dict[name + '.weight'].shape)\n",
        "                weight = oristate_dict[name + '.weight'].data\n",
        "                similarity_matrix = np.zeros((orifilter_num, orifilter_num))\n",
        "                for i in range(orifilter_num):\n",
        "                  for j in range(orifilter_num):\n",
        "                    # print(f'Computing the distance between filter {i} and filter {j}:')\n",
        "                    dist = torch.dist(weight[i], weight[j])\n",
        "                    similarity_matrix[i, j] = dist\n",
        "                    # print(dist)\n",
        "\n",
        "                print(similarity_matrix)\n",
        "                row_sums = np.sum(similarity_matrix, axis=1) # compute the sum of the distance of 1 filter to all other filters\n",
        "                rank = row_sums\n",
        "                #********************\n",
        "                # print(f\"rank {rank}\")\n",
        "                select_index = np.argsort(\n",
        "                    rank)[orifilter_num-currentfilter_num:]  # preserved filter id\n",
        "                select_index.sort()\n",
        "\n",
        "                if last_select_index is not None:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        for index_j, j in enumerate(last_select_index):\n",
        "                            state_dict[name + '.weight'][index_i][index_j] = \\\n",
        "                                oristate_dict[name + '.weight'][i][j]\n",
        "                else:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        state_dict[name + '.weight'][index_i] = \\\n",
        "                            oristate_dict[name + '.weight'][i]\n",
        "\n",
        "                last_select_index = select_index\n",
        "\n",
        "            elif last_select_index is not None:\n",
        "                for i in range(orifilter_num):\n",
        "                    for index_j, j in enumerate(last_select_index):\n",
        "                        state_dict[name + '.weight'][i][index_j] = \\\n",
        "                            oristate_dict[name + '.weight'][i][j]\n",
        "            else:\n",
        "                state_dict[name + '.weight'] = oriweight\n",
        "                last_select_index = None\n",
        "\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "CJ8Od02vpSKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_similarity(model_prune, model_ori)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFx3scSwp386",
        "outputId": "2e13b782-8e06-4e59-e3cf-bf0d6ee90236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing layer 1, original layer has 64 filters, pruning model has 48 filters\n",
            "[[0.         1.74139035 1.59087288 ... 1.72806299 1.57414818 1.61237991]\n",
            " [1.74139035 0.         1.0449996  ... 1.0066036  0.88404632 0.85878301]\n",
            " [1.59087288 1.0449996  0.         ... 0.66931009 0.51807326 0.4106819 ]\n",
            " ...\n",
            " [1.72806299 1.0066036  0.66931009 ... 0.         0.57510459 0.59303981]\n",
            " [1.57414818 0.88404632 0.51807326 ... 0.57510459 0.         0.43026134]\n",
            " [1.61237991 0.85878301 0.4106819  ... 0.59303981 0.43026134 0.        ]]\n",
            "Processing layer 2, original layer has 64 filters, pruning model has 48 filters\n",
            "[[0.         1.18950224 1.54022479 ... 1.15816128 1.24496245 1.15282464]\n",
            " [1.18950224 0.         1.11939645 ... 1.11556244 0.97027659 1.26536036]\n",
            " [1.54022479 1.11939645 0.         ... 1.34018302 1.27003932 1.45868313]\n",
            " ...\n",
            " [1.15816128 1.11556244 1.34018302 ... 0.         1.18896043 1.27326846]\n",
            " [1.24496245 0.97027659 1.27003932 ... 1.18896043 0.         1.27268112]\n",
            " [1.15282464 1.26536036 1.45868313 ... 1.27326846 1.27268112 0.        ]]\n",
            "Processing layer 3, original layer has 128 filters, pruning model has 96 filters\n",
            "[[0.         1.42122185 1.22746193 ... 1.27757788 1.25067925 1.32484221]\n",
            " [1.42122185 0.         1.49760747 ... 1.40180743 1.3444494  1.55740595]\n",
            " [1.22746193 1.49760747 0.         ... 1.14871061 1.15785098 1.2968961 ]\n",
            " ...\n",
            " [1.27757788 1.40180743 1.14871061 ... 0.         1.02785122 1.15371168]\n",
            " [1.25067925 1.3444494  1.15785098 ... 1.02785122 0.         1.14477229]\n",
            " [1.32484221 1.55740595 1.2968961  ... 1.15371168 1.14477229 0.        ]]\n",
            "Processing layer 4, original layer has 128 filters, pruning model has 96 filters\n",
            "[[0.         1.46633482 1.60315692 ... 1.24838531 1.58495939 1.38703716]\n",
            " [1.46633482 0.         1.58788896 ... 1.38457549 1.64786541 1.40805209]\n",
            " [1.60315692 1.58788896 0.         ... 1.45373452 1.7336154  1.57224369]\n",
            " ...\n",
            " [1.24838531 1.38457549 1.45373452 ... 0.         1.55818081 1.32155156]\n",
            " [1.58495939 1.64786541 1.7336154  ... 1.55818081 0.         1.58488214]\n",
            " [1.38703716 1.40805209 1.57224369 ... 1.32155156 1.58488214 0.        ]]\n",
            "Processing layer 5, original layer has 256 filters, pruning model has 192 filters\n",
            "[[0.         1.29846847 1.33431685 ... 1.22870421 1.09841478 1.34573019]\n",
            " [1.29846847 0.         1.39555585 ... 1.29792893 1.18036664 1.33800352]\n",
            " [1.33431685 1.39555585 0.         ... 1.39792633 1.19068551 1.36408675]\n",
            " ...\n",
            " [1.22870421 1.29792893 1.39792633 ... 0.         1.13475263 1.32387829]\n",
            " [1.09841478 1.18036664 1.19068551 ... 1.13475263 0.         1.16552055]\n",
            " [1.34573019 1.33800352 1.36408675 ... 1.32387829 1.16552055 0.        ]]\n",
            "Processing layer 6, original layer has 256 filters, pruning model has 192 filters\n",
            "[[0.         1.49483824 1.47098637 ... 1.41265988 1.34934354 1.46011519]\n",
            " [1.49483824 0.         1.3395977  ... 1.23842406 1.24481761 1.27968168]\n",
            " [1.47098637 1.3395977  0.         ... 1.26577151 1.2824136  1.35073209]\n",
            " ...\n",
            " [1.41265988 1.23842406 1.26577151 ... 0.         1.18796015 1.24796057]\n",
            " [1.34934354 1.24481761 1.2824136  ... 1.18796015 0.         1.26724339]\n",
            " [1.46011519 1.27968168 1.35073209 ... 1.24796057 1.26724339 0.        ]]\n",
            "Processing layer 7, original layer has 256 filters, pruning model has 192 filters\n",
            "[[0.         1.11892903 1.15290225 ... 1.22711694 1.13740611 1.08098245]\n",
            " [1.11892903 0.         1.21141601 ... 1.24150646 1.23232353 1.17370617]\n",
            " [1.15290225 1.21141601 0.         ... 1.24543631 1.20642948 1.13828218]\n",
            " ...\n",
            " [1.22711694 1.24150646 1.24543631 ... 0.         1.29025269 1.21557927]\n",
            " [1.13740611 1.23232353 1.20642948 ... 1.29025269 0.         1.1358875 ]\n",
            " [1.08098245 1.17370617 1.13828218 ... 1.21557927 1.1358875  0.        ]]\n",
            "Processing layer 8, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.74783897 0.53641456 ... 0.65880537 0.69070882 0.62650144]\n",
            " [0.74783897 0.         0.70040119 ... 0.8114686  0.83108282 0.81882209]\n",
            " [0.53641456 0.70040119 0.         ... 0.64930391 0.66257775 0.65391576]\n",
            " ...\n",
            " [0.65880537 0.8114686  0.64930391 ... 0.         0.74297321 0.72759873]\n",
            " [0.69070882 0.83108282 0.66257775 ... 0.74297321 0.         0.77549708]\n",
            " [0.62650144 0.81882209 0.65391576 ... 0.72759873 0.77549708 0.        ]]\n",
            "Processing layer 9, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.44937032 0.44717467 ... 0.44214386 0.42544821 0.39422199]\n",
            " [0.44937032 0.         0.43297523 ... 0.45674026 0.46420765 0.44007283]\n",
            " [0.44717467 0.43297523 0.         ... 0.34375292 0.3272014  0.3267366 ]\n",
            " ...\n",
            " [0.44214386 0.45674026 0.34375292 ... 0.         0.28739169 0.33429554]\n",
            " [0.42544821 0.46420765 0.3272014  ... 0.28739169 0.         0.35326961]\n",
            " [0.39422199 0.44007283 0.3267366  ... 0.33429554 0.35326961 0.        ]]\n",
            "Processing layer 10, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.24060258 0.24136184 ... 0.23888567 0.34078321 0.31026241]\n",
            " [0.24060258 0.         0.20055246 ... 0.23511079 0.30632436 0.26569974]\n",
            " [0.24136184 0.20055246 0.         ... 0.20995007 0.27098751 0.2573823 ]\n",
            " ...\n",
            " [0.23888567 0.23511079 0.20995007 ... 0.         0.30488899 0.26040235]\n",
            " [0.34078321 0.30632436 0.27098751 ... 0.30488899 0.         0.31724426]\n",
            " [0.31026241 0.26569974 0.2573823  ... 0.26040235 0.31724426 0.        ]]\n",
            "Processing layer 11, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.25313532 0.15862787 ... 0.18030564 0.16444081 0.12301059]\n",
            " [0.25313532 0.         0.23972732 ... 0.29272699 0.26680851 0.23072371]\n",
            " [0.15862787 0.23972732 0.         ... 0.17284445 0.13971126 0.11121501]\n",
            " ...\n",
            " [0.18030564 0.29272699 0.17284445 ... 0.         0.1739077  0.1441773 ]\n",
            " [0.16444081 0.26680851 0.13971126 ... 0.1739077  0.         0.12007354]\n",
            " [0.12301059 0.23072371 0.11121501 ... 0.1441773  0.12007354 0.        ]]\n",
            "Processing layer 12, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.15282945 0.1384365  ... 0.17996098 0.16928835 0.12805164]\n",
            " [0.15282945 0.         0.09452864 ... 0.1907845  0.17421071 0.15122023]\n",
            " [0.1384365  0.09452864 0.         ... 0.15639731 0.16523457 0.10234247]\n",
            " ...\n",
            " [0.17996098 0.1907845  0.15639731 ... 0.         0.17768517 0.1722673 ]\n",
            " [0.16928835 0.17421071 0.16523457 ... 0.17768517 0.         0.1745384 ]\n",
            " [0.12805164 0.15122023 0.10234247 ... 0.1722673  0.1745384  0.        ]]\n",
            "Processing layer 13, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.17427406 0.19477567 ... 0.11588519 0.18578708 0.12594552]\n",
            " [0.17427406 0.         0.23022732 ... 0.13994159 0.21605481 0.17047261]\n",
            " [0.19477567 0.23022732 0.         ... 0.19367294 0.23759399 0.15547395]\n",
            " ...\n",
            " [0.11588519 0.13994159 0.19367294 ... 0.         0.16849667 0.10820013]\n",
            " [0.18578708 0.21605481 0.23759399 ... 0.16849667 0.         0.14886674]\n",
            " [0.12594552 0.17047261 0.15547395 ... 0.10820013 0.14886674 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the model after pruning, without finetuning:\")\n",
        "_, accuracy_model_prune, _ = validate(val_loader, model_prune, criterion)\n",
        "print(f\"This model's accuracy is {accuracy_model_prune}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Bn9qQAu9gv",
        "outputId": "75f6b33a-3a72-4a1d-8514-7ab7abd2f0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model after pruning, without finetuning:\n",
            " * Acc@1 10.000 Acc@5 50.020\n",
            "This model's accuracy is 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(model_prune, train_loader, val_loader, epochs=1, criterion=criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyLGmopFvAld",
        "outputId": "f6575d71-8081-4520-ce20-ad4efd405e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 10.000 Acc@5 50.020\n",
            "learning_rate: 0.0001\n",
            "Epoch[0](0/391): Loss 2.1867 Prec@1(1,5) 20.31, 67.19 Lr 0.0001\n",
            "Epoch[0](39/391): Loss 1.9700 Prec@1(1,5) 33.50, 78.67 Lr 0.0001\n",
            "Epoch[0](78/391): Loss 1.7097 Prec@1(1,5) 49.00, 85.36 Lr 0.0001\n",
            "Epoch[0](117/391): Loss 1.5341 Prec@1(1,5) 55.89, 88.79 Lr 0.0001\n",
            "Epoch[0](156/391): Loss 1.4014 Prec@1(1,5) 60.56, 90.80 Lr 0.0001\n",
            "Epoch[0](195/391): Loss 1.2996 Prec@1(1,5) 63.79, 92.12 Lr 0.0001\n",
            "Epoch[0](234/391): Loss 1.2187 Prec@1(1,5) 66.18, 93.06 Lr 0.0001\n",
            "Epoch[0](273/391): Loss 1.1466 Prec@1(1,5) 68.31, 93.80 Lr 0.0001\n",
            "Epoch[0](312/391): Loss 1.0864 Prec@1(1,5) 69.98, 94.41 Lr 0.0001\n",
            "Epoch[0](351/391): Loss 1.0340 Prec@1(1,5) 71.46, 94.85 Lr 0.0001\n",
            "Epoch[0](390/391): Loss 0.9885 Prec@1(1,5) 72.69, 95.26 Lr 0.0001\n",
            " * Acc@1 80.640 Acc@5 98.350\n",
            "=>Best accuracy 80.640\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu4): ReLU(inplace=True)\n",
              "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu6): ReLU(inplace=True)\n",
              "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu7): ReLU(inplace=True)\n",
              "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu8): ReLU(inplace=True)\n",
              "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu10): ReLU(inplace=True)\n",
              "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu11): ReLU(inplace=True)\n",
              "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu12): ReLU(inplace=True)\n",
              "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu14): ReLU(inplace=True)\n",
              "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu15): ReLU(inplace=True)\n",
              "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu16): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
              "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision opencv-python\n"
      ],
      "metadata": {
        "id": "lBLwTr_Z4acy",
        "outputId": "bf4269ea-60ed-4466-9257-cd44fa5cad6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.transforms as transforms\n",
        "# from torchvision.models import vgg16_bn\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# class CustomVGG16(nn.Module):\n",
        "#     def __init__(self, original_vgg16):\n",
        "#         super(CustomVGG16, self).__init__()\n",
        "#         self.features = original_vgg16.features\n",
        "#         self.avgpool = original_vgg16.avgpool\n",
        "#         self.classifier = original_vgg16.classifier\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.features(x)\n",
        "#         x = self.avgpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "# # Load pre-trained VGG16 models\n",
        "# model_ori = vgg16_bn(weights=\"IMAGENET1K_V1\").cuda()\n",
        "# model_prune = vgg16_bn(weights=None).cuda()  # Assuming the pruned model definition is the same\n",
        "\n",
        "# # Wrap the models in CustomVGG16 to ensure proper handling of input dimensions\n",
        "# model_ori = CustomVGG16(model_ori)\n",
        "# model_prune = CustomVGG16(model_prune)\n",
        "\n",
        "# # Load the video\n",
        "# video_path = '/content/video.mp4'\n",
        "# cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# # Video writer to save the outputs\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "# out_original = cv2.VideoWriter('original_output.avi', fourcc, 20.0, (640, 480))\n",
        "# out_pruned = cv2.VideoWriter('pruned_output.avi', fourcc, 20.0, (640, 480))\n",
        "\n",
        "# # Preprocessing transformation\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToPILImage(),\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# def preprocess_frame(frame):\n",
        "#     frame_tensor = transform(frame).unsqueeze(0).cuda()\n",
        "#     return frame_tensor\n",
        "\n",
        "# # Function to run inference\n",
        "# def run_inference(model, frame):\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         output = model(frame)\n",
        "#     return output\n",
        "\n",
        "# # Inference and comparison loop\n",
        "# while cap.isOpened():\n",
        "#     ret, frame = cap.read()\n",
        "#     if not ret:\n",
        "#         break\n",
        "\n",
        "#     # Preprocess the frame\n",
        "#     preprocessed_frame = preprocess_frame(frame)\n",
        "\n",
        "#     # Run inference\n",
        "#     original_output = run_inference(model_ori, preprocessed_frame)\n",
        "#     pruned_output = run_inference(model_prune, preprocessed_frame)\n",
        "\n",
        "#     # Assuming the output is a classification, get the predicted class\n",
        "#     _, original_pred = torch.max(original_output, 1)\n",
        "#     _, pruned_pred = torch.max(pruned_output, 1)\n",
        "\n",
        "#     # Overlay predictions on the frame (dummy example)\n",
        "#     frame_original = frame.copy()\n",
        "#     frame_pruned = frame.copy()\n",
        "\n",
        "#     # Display the predictions (dummy text overlay)\n",
        "#     cv2.putText(frame_original, f'Original: {original_pred.item()}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "#     cv2.putText(frame_pruned, f'Pruned: {pruned_pred.item()}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "#     # Write the frames with detections\n",
        "#     out_original.write(frame_original)\n",
        "#     out_pruned.write(frame_pruned)\n",
        "\n",
        "#     # Display the frames using cv2_imshow\n",
        "#     cv2_imshow(frame_original)\n",
        "#     cv2_imshow(frame_pruned)\n",
        "\n",
        "#     # Optional delay between frames\n",
        "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         break\n",
        "\n",
        "# cap.release()\n",
        "# out_original.release()\n",
        "# out_pruned.release()\n",
        "# cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import cv2\n",
        "# import torch\n",
        "# import torchvision.transforms as transforms\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import time\n",
        "\n",
        "# # Function to extract frames from video\n",
        "# def extract_frames(video_path, resize_shape=(224, 224)):\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "#     frames = []\n",
        "#     while cap.isOpened():\n",
        "#         ret, frame = cap.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "#         frame = cv2.resize(frame, resize_shape)\n",
        "#         frames.append(frame)\n",
        "#     cap.release()\n",
        "#     return frames\n",
        "\n",
        "# # Function to run inference and calculate FPS\n",
        "# def run_inference(frames, model, device):\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.ToPILImage(),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#     ])\n",
        "#     fps_list = []\n",
        "#     results = []\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for frame in frames:\n",
        "#             image = transform(frame).unsqueeze(0).to(device)\n",
        "#             start_time = time.time()\n",
        "#             outputs = model(image)\n",
        "#             end_time = time.time()\n",
        "#             fps = 1 / (end_time - start_time)\n",
        "#             fps_list.append(fps)\n",
        "#             results.append(outputs.cpu().numpy())\n",
        "#     avg_fps = np.mean(fps_list)\n",
        "#     return results, avg_fps\n",
        "\n",
        "# # Function to visualize results\n",
        "# def visualize_results(frames, results, fps, title):\n",
        "#     for i, frame in enumerate(frames):\n",
        "#         result = results[i]\n",
        "#         # Add code to draw bounding boxes and labels based on the results\n",
        "#         # For simplicity, we'll just display the frame and FPS here\n",
        "#         cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "#         cv2.imshow(title, frame)\n",
        "#         if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "#             break\n",
        "#     cv2.destroyAllWindows()\n",
        "\n",
        "# # Load the video\n",
        "# video_path = '/content/video.mp4'\n",
        "# frames = extract_frames(video_path)\n",
        "\n",
        "# # Ensure device is set\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model_ori = model_ori.to(device)\n",
        "# model_prune = model_prune.to(device)\n",
        "\n",
        "# # Run inference and measure FPS\n",
        "# results_ori, fps_ori = run_inference(frames, model_ori, device)\n",
        "# results_prune, fps_prune = run_inference(frames, model_prune, device)\n",
        "\n",
        "# # Visualize results\n",
        "# visualize_results(frames, results_ori, fps_ori, \"Original Model\")\n",
        "# visualize_results(frames, results_prune, fps_prune, \"Pruned Model\")\n",
        "\n",
        "# print(f\"Original Model FPS: {fps_ori}\")\n",
        "# print(f\"Pruned Model FPS: {fps_prune}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kjebuk_m5EZA",
        "outputId": "f506f48f-c052-42ca-ace3-b6e97287259c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_ori' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fc3e1ef8e448>\u001b[0m in \u001b[0;36m<cell line: 164>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m# Ensure device is set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mmodel_ori\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0mmodel_prune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_prune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_ori' is not defined"
          ]
        }
      ]
    }
  ]
}